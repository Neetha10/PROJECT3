{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Task 1: Basics – Evaluate Pretrained ResNet-34 on ImageNet Subset\n",
        "\n",
        " Task Overview\n",
        "In this initial task, we evaluate a pretrained ResNet-34 model on a test subset of the ImageNet-1K dataset. This subset includes images from 100 selected classes, and our objective is to:\n",
        "\n",
        "Load the images and labels.\n",
        "\n",
        "Preprocess the data using ImageNet's standard normalization.\n",
        "\n",
        "Use the pretrained ResNet-34 model from torchvision to make predictions.\n",
        "\n",
        "Compute Top-1 and Top-5 accuracy for the model.\n",
        "\n",
        "This task establishes a baseline performance that will later be degraded using adversarial attacks."
      ],
      "metadata": {
        "id": "m7Ttq_F3YNUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Importing Required Libraries\n",
        "This cell imports all the necessary libraries for handling files, image processing, dataset loading, and deep learning using PyTorch."
      ],
      "metadata": {
        "id": "JLqzjDa0YgPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries for data handling, image transformation, and model inference\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "metadata": {
        "id": "gZUq0Y90O8VX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Loading Class Labels from JSON\n",
        "\n",
        "This cell loads the class labels from the labels_list.json file, which maps each selected class to its corresponding ImageNet index."
      ],
      "metadata": {
        "id": "Kk6lclYBYkxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the list of labels (e.g., \"401: accordion\") from the provided JSON file\n",
        "with open('labels_list.json', 'r') as f:\n",
        "    labels_json = json.load(f)\n"
      ],
      "metadata": {
        "id": "-lLOD7rUPi2v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Verifying Loaded Labels\n",
        "\n",
        "This cell prints out the number of labels loaded and previews the first few entries to verify correct formatting."
      ],
      "metadata": {
        "id": "pw4ZErbEYr9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of entries and sample the first 5 to inspect formatting\n",
        "print(f\"Loaded {len(labels_json)} labels from JSON file\")\n",
        "print(\"First 5 entries:\", labels_json[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN3mF3NlPks7",
        "outputId": "ba71175c-2860-44f7-fa28-e3c477b90e6c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 labels from JSON file\n",
            "First 5 entries: ['401: accordion', '402: acoustic guitar', '403: aircraft carrier', '404: airliner', '405: airship']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Parsing Class IDs and Names\n",
        "\n",
        "Here we parse each label entry to extract the ImageNet class index and human-readable class name."
      ],
      "metadata": {
        "id": "btnLJBxkYx52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate each label into class ID (int) and class name (string)\n",
        "class_ids = []\n",
        "class_names = []\n",
        "for entry in labels_json:\n",
        "    parts = entry.split(':', 1)\n",
        "    if len(parts) == 2:\n",
        "        class_id = int(parts[0].strip())\n",
        "        class_name = parts[1].strip()\n",
        "        class_ids.append(class_id)\n",
        "        class_names.append(class_name)\n"
      ],
      "metadata": {
        "id": "blQfUq_SPmhn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Verifying Parsed Class Data\n",
        "\n",
        "This quick check ensures we successfully extracted all 100 class IDs and names."
      ],
      "metadata": {
        "id": "spJYMQaoY2dP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Extracted {len(class_ids)} class IDs and names\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQBu5DmuPo3H",
        "outputId": "33e60bf1-ca84-44a9-9298-933533b25929"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 100 class IDs and names\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Check if the directory already exists\n",
        "if not os.path.exists(\"./TestDataSet\"):\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(\"TestDataSet.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "    print(\"Extracted TestDataSet.zip successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JxcRqYfCrMD",
        "outputId": "a17107c8-5158-474e-93c1-15ab79d56dd4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted TestDataSet.zip successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Detecting Dataset Folders\n",
        "\n",
        "We now scan the TestDataSet directory and list all class folders that begin with 'n' — the standard ImageNet folder prefix."
      ],
      "metadata": {
        "id": "PWZD97oBY-p_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect folders corresponding to image classes (e.g., \"n02106662\")\n",
        "dataset_path = \"./TestDataSet\"\n",
        "folders = [d for d in os.listdir(dataset_path)\n",
        "           if os.path.isdir(os.path.join(dataset_path, d)) and d.startswith('n')]\n",
        "folders.sort()  # Ensure consistent order\n"
      ],
      "metadata": {
        "id": "We5KtMOcPrV-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Previewing Folder Names\n",
        "\n",
        "This helps us verify that the folder structure was detected correctly by printing a count and previewing a few folder names."
      ],
      "metadata": {
        "id": "WXHzkma4ZCkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print number of folders found and preview some of them\n",
        "print(f\"Found {len(folders)} folders in the dataset\")\n",
        "print(\"First 5 folders:\", folders[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRWHG4uZPtk6",
        "outputId": "4d60924b-ebee-43b4-ac62-75cac70060b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 folders in the dataset\n",
            "First 5 folders: ['n02672831', 'n02676566', 'n02687172', 'n02690373', 'n02692877']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Mapping Folders to Class Labels\n",
        "\n",
        "Each folder is mapped to a class index using aligned order. A mismatch in folder/label count is also handled with a warning."
      ],
      "metadata": {
        "id": "n-6eoRxoZHem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_to_label = {}\n",
        "if len(folders) == len(class_ids):\n",
        "    # Directly map folders to class IDs based on their order\n",
        "    for i, folder in enumerate(folders):\n",
        "        folder_to_label[folder] = class_ids[i]\n",
        "    print(\"Created folder to label mapping based on order\")\n",
        "else:\n",
        "    print(f\"Warning: Mismatch in counts - {len(folders)} folders vs {len(class_ids)} labels\")\n",
        "    # Still create a mapping but with a warning\n",
        "    min_length = min(len(folders), len(class_ids))\n",
        "    for i in range(min_length):\n",
        "        folder_to_label[folders[i]] = class_ids[i]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Waa9sZMNPvn7",
        "outputId": "c1d3d8d0-01b1-465f-afdc-b985952c4074"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created folder to label mapping based on order\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Saving Mappings to JSON\n",
        "\n",
        "This cell saves both the folder-to-label and folder-to-name mappings to a JSON file for reuse in future evaluation steps."
      ],
      "metadata": {
        "id": "tdz_mzfIZNIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save mappings as a JSON file for consistent reference across experiments\n",
        "mapping_dict = {\n",
        "    'folder_to_label': folder_to_label,\n",
        "    'folder_to_name': {folders[i]: class_names[i] for i in range(min(len(folders), len(class_names)))}\n",
        "}\n",
        "with open('imagenet_mapping.json', 'w') as f:\n",
        "    json.dump(mapping_dict, f, indent=4)\n",
        "print(\"Saved mapping to imagenet_mapping.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAenhqr0Py0E",
        "outputId": "3f1f4093-6947-4bee-b709-398a687b06aa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved mapping to imagenet_mapping.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10: Creating a Custom Dataset Class\n",
        "\n",
        "We define a custom OrderedImageNetDataset class that uses our folder-to-label mapping to load image-label pairs in a structure compatible with PyTorch’s DataLoader."
      ],
      "metadata": {
        "id": "vNH4I-hTZSBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom dataset class using this mapping\n",
        "class OrderedImageNetDataset(Dataset):\n",
        "    def __init__(self, root_dir, folder_to_label, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.folder_to_label = folder_to_label\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect all image paths and their labels\n",
        "        self.samples = []\n",
        "        for folder, label in folder_to_label.items():\n",
        "            folder_path = os.path.join(root_dir, folder)\n",
        "            if os.path.isdir(folder_path):\n",
        "                for img_name in os.listdir(folder_path):\n",
        "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        img_path = os.path.join(folder_path, img_name)\n",
        "                        self.samples.append((img_path, label))\n",
        "\n",
        "        print(f\"Loaded {len(self.samples)} images from {len(folder_to_label)} class folders\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "43lFHslOP2W8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 11: Defining Image Transforms\n",
        "\n",
        "This cell defines the preprocessing pipeline for input images using standard ImageNet normalization values. Images are resized, center cropped to 224×224, converted to tensors, and normalized."
      ],
      "metadata": {
        "id": "TRfkAZ_MZgM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Define transforms as given in the project PDF\n",
        "mean_norms = np.array([0.485, 0.456, 0.406])\n",
        "std_norms = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "plain_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean_norms, std=std_norms)\n",
        "])"
      ],
      "metadata": {
        "id": "ZNkyqIblQABa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 12: Creating the Dataset and DataLoader\n",
        "\n",
        "This step initializes our custom dataset using the image folder and applies the defined transforms. It then wraps it in a DataLoader to allow efficient batching and iteration."
      ],
      "metadata": {
        "id": "xQnwFb95Zl5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Create dataset and data loader\n",
        "dataset = OrderedImageNetDataset(\n",
        "    root_dir=dataset_path,\n",
        "    folder_to_label=folder_to_label,\n",
        "    transform=plain_transforms\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frHDAKjtP62s",
        "outputId": "a41e35ef-d694-4d16-b6db-f3104db2bc46"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 500 images from 100 class folders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 13: Loading the Pretrained ResNet-34 Model\n",
        "\n",
        "Here we load the pretrained ResNet-34 model from torchvision, set it to evaluation mode, and move it to GPU if available."
      ],
      "metadata": {
        "id": "5LwS5PdiZrah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained ResNet-34 as specified in the project\n",
        "model = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
        "model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "cRnc3WOQQFix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0427ca37-fcd7-4e7f-d926-9cbf603bfa0d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 227MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 14: Defining the Evaluation Function\n",
        "\n",
        "This function computes the Top-1 and Top-5 accuracy of the model on our dataset. Top-1 means the most confident prediction is correct. Top-5 means the correct label is in the top 5 predictions."
      ],
      "metadata": {
        "id": "I8a1R-n4aZR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Define evaluation function as required in Task 1\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct_top1 = 0\n",
        "    correct_top5 = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Top-1 accuracy\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct_top1 += (predicted == labels).sum().item()\n",
        "\n",
        "            # Top-5 accuracy\n",
        "            _, top5_indices = outputs.topk(5, dim=1)\n",
        "            for i in range(labels.size(0)):\n",
        "                if labels[i] in top5_indices[i]:\n",
        "                    correct_top5 += 1\n",
        "\n",
        "            total += labels.size(0)\n",
        "\n",
        "    top1_accuracy = 100 * correct_top1 / total\n",
        "    top5_accuracy = 100 * correct_top5 / total\n",
        "\n",
        "    return top1_accuracy, top5_accuracy"
      ],
      "metadata": {
        "id": "uv33JJpOQIAl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 15: Running Evaluation and Printing Results\n",
        "\n",
        "This step runs the model evaluation on the full dataset and prints the resulting top-1 and top-5 accuracy, which serve as our baseline for the upcoming adversarial attacks."
      ],
      "metadata": {
        "id": "pOTrbdOuadWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Evaluate the model as required in Task 1\n",
        "print(f\"Evaluating ResNet-34 on {len(dataset)} images...\")\n",
        "top1_acc, top5_acc = evaluate_model(model, dataloader, device)\n",
        "print(f\"Top-1 Accuracy: {top1_acc:.2f}%\")\n",
        "print(f\"Top-5 Accuracy: {top5_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPhWB9y-QMZX",
        "outputId": "ac3fa59c-88c2-4f52-a2fa-e2a49feea409"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating ResNet-34 on 500 images...\n",
            "Top-1 Accuracy: 70.60%\n",
            "Top-5 Accuracy: 93.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Pixel-wise Adversarial Attacks using FGSM\n",
        "\n",
        "Task Description\n",
        "\n",
        "In this task, we implement the Fast Gradient Sign Method (FGSM) — a simple yet powerful adversarial attack technique. The idea is to perturb each input image slightly in the direction of the gradient of the loss function with respect to the input pixels, effectively \"nudging\" the image in a way that confuses the classifier.\n",
        "We constrain the perturbation using an L∞ norm, so that no individual pixel changes too much. The goal is to degrade model accuracy while keeping the adversarial images visually similar to the originals."
      ],
      "metadata": {
        "id": "mKzref28anT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import Required Libraries for Attacks and Visualization\n",
        "\n",
        "This cell imports additional libraries required for visualizing, saving images, and computing attacks."
      ],
      "metadata": {
        "id": "mMIOh8Kqawn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional libraries for adversarial attack and visualization\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F  # For loss computation\n",
        "import torchvision.utils as vutils  # For saving image grids\n"
      ],
      "metadata": {
        "id": "FDbZKduKRObp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Create Output Directory for Adversarial Images\n",
        "\n",
        "We define a directory where we’ll save all generated adversarial images."
      ],
      "metadata": {
        "id": "Ytrl4UfOa0wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory to save the adversarial images\n",
        "SAVE_PATH = \"./AdversarialTestSet1\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)"
      ],
      "metadata": {
        "id": "Ak9KdmBFRPHM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Create Class Folder Structure\n",
        "\n",
        "This replicates the original folder structure (one folder per class) inside the adversarial output directory."
      ],
      "metadata": {
        "id": "u8uguhjua4xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy folder structure from original dataset\n",
        "for folder in folders:\n",
        "    os.makedirs(os.path.join(SAVE_PATH, folder), exist_ok=True)"
      ],
      "metadata": {
        "id": "W_TNNMc1RQ96"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Define a Modified Dataset Class for FGSM\n",
        "\n",
        "This dataset class is extended to return extra metadata — like folder and image filename — so we can save the generated adversarial image in the right location."
      ],
      "metadata": {
        "id": "VPoa9nLya7Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class modified to return filename and folder info for saving adversarial images\n",
        "class FGSMDataset(Dataset):\n",
        "    def __init__(self, root_dir, folder_to_label, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.folder_to_label = folder_to_label\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "\n",
        "        # Gather image paths and related info\n",
        "        for folder, label in folder_to_label.items():\n",
        "            folder_path = os.path.join(root_dir, folder)\n",
        "            if os.path.isdir(folder_path):\n",
        "                for img_name in os.listdir(folder_path):\n",
        "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        img_path = os.path.join(folder_path, img_name)\n",
        "                        self.samples.append((img_path, label, folder, img_name))\n",
        "\n",
        "        print(f\"Loaded {len(self.samples)} images from {len(folder_to_label)} class folders\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return image, label, and metadata for saving\n",
        "        img_path, label, folder, img_name = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label, folder, img_name\n"
      ],
      "metadata": {
        "id": "rQ_o7AN5RTZY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Instantiate FGSM Dataset\n",
        "\n",
        "This creates the FGSMDataset instance using our existing image folders and normalization transforms."
      ],
      "metadata": {
        "id": "rbzzAXmHa_vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset specifically for the FGSM attack\n",
        "attack_dataset = FGSMDataset(\n",
        "    root_dir=dataset_path,\n",
        "    folder_to_label=folder_to_label,\n",
        "    transform=plain_transforms\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9243J8SzRXi9",
        "outputId": "2cdd4243-98c8-4b77-8ec5-54200772a895"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 500 images from 100 class folders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Create a DataLoader with Batch Size 1\n",
        "\n",
        "FGSM is typically applied per image, so we use a batch size of 1 to simplify processing and saving."
      ],
      "metadata": {
        "id": "aN7ZbqIDbGlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use batch size of 1 to simplify attack implementation\n",
        "attack_loader = DataLoader(attack_dataset, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "d25Bqz-_RavJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Define the FGSM Attack Function\n",
        "\n",
        "This function perturbs the image by taking a single step in the direction of the sign of the gradient."
      ],
      "metadata": {
        "id": "qnuYC2P9bJlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "    # Get the sign of the gradient\n",
        "    sign_data_grad = torch.sign(data_grad)\n",
        "\n",
        "    # Create the perturbed image\n",
        "    perturbed_image = image + epsilon * sign_data_grad\n",
        "\n",
        "    # Return the perturbed image\n",
        "    return perturbed_image\n"
      ],
      "metadata": {
        "id": "-KAzrUJRRdIA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Compute L∞ Distance\n",
        "\n",
        "We use this utility to verify that the perturbation respects the ε constraint — no pixel is changed by more than ε."
      ],
      "metadata": {
        "id": "I-1fPexYbMq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute L∞ distance\n",
        "def l_inf_distance(original, perturbed):\n",
        "    # Calculate the maximum absolute difference\n",
        "    return (original - perturbed).abs().max().item()"
      ],
      "metadata": {
        "id": "2NvSwHdBRfEf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Denormalize and Save Image to Disk\n",
        "\n",
        "This helper function unnormalizes a tensor image back to RGB and saves it to disk as a PNG or JPEG."
      ],
      "metadata": {
        "id": "WQiSBbVBbR1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to denormalize and save images\n",
        "def save_normalized_image(tensor, path, mean, std):\n",
        "    # Create a copy of the tensor\n",
        "    img = tensor.clone()\n",
        "\n",
        "    # Unnormalize the image\n",
        "    for t, m, s in zip(img, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "\n",
        "    # Ensure the pixel values are in [0, 1]\n",
        "    img = torch.clamp(img, 0, 1)\n",
        "\n",
        "    # Save the image\n",
        "    vutils.save_image(img, path)"
      ],
      "metadata": {
        "id": "-oDa-6PiRhL5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10: Visualize Adversarial Attack Example\n",
        "\n",
        "This function shows the original, adversarial, and difference image side-by-side, and saves them as PNG files."
      ],
      "metadata": {
        "id": "h9pfwQCIbVki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize and save an attack example showing original, adversarial, and difference\n",
        "def visualize_attack(original, adversarial, original_label, adversarial_label,\n",
        "                     original_class, adversarial_class, index):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # -- Original Image --\n",
        "    plt.subplot(1, 3, 1)\n",
        "    img = original.cpu().squeeze()\n",
        "    img_display = img.clone()\n",
        "    for t, m, s in zip(img_display, mean_norms, std_norms):\n",
        "        t.mul_(s).add_(m)\n",
        "    img_display = img_display.permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "    plt.imshow(img_display)\n",
        "    plt.title(f\"Original\\nLabel: {original_label}\\nClass: {original_class}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # -- Adversarial Image --\n",
        "    plt.subplot(1, 3, 2)\n",
        "    adv_img = adversarial.cpu().squeeze()\n",
        "    adv_img_display = adv_img.clone()\n",
        "    for t, m, s in zip(adv_img_display, mean_norms, std_norms):\n",
        "        t.mul_(s).add_(m)\n",
        "    adv_img_display = adv_img_display.permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "    plt.imshow(adv_img_display)\n",
        "    plt.title(f\"Adversarial\\nLabel: {adversarial_label}\\nClass: {adversarial_class}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # -- Difference Image --\n",
        "    plt.subplot(1, 3, 3)\n",
        "    diff = np.abs(adv_img_display - img_display) * 10  # Amplified for visibility\n",
        "    plt.imshow(diff)\n",
        "    plt.title(\"Difference (x10)\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'attack_example_{index}.png')\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "WpsO4XpSRjto"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 11: Define a Helper to Get Class Names\n",
        "\n",
        "This utility function maps numeric class labels back to human-readable class names using the saved mapping_dict."
      ],
      "metadata": {
        "id": "3AX3MTUpbcGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given a class label index, return its corresponding human-readable class name\n",
        "def get_class_name(label):\n",
        "    for folder, l in folder_to_label.items():\n",
        "        if l == label:\n",
        "            return mapping_dict['folder_to_name'].get(folder, str(label))\n",
        "    return str(label)  # fallback\n"
      ],
      "metadata": {
        "id": "jS2Msyh_RmNy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 12: Set Epsilon for FGSM\n",
        "\n",
        "We define the ε value that controls the strength of the FGSM attack. This limits how much each pixel can be changed.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3MTD_VcVbiPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the maximum pixel-wise perturbation for FGSM\n",
        "epsilon = 0.02  # Conforms to ±1 in [0, 255] pixel range\n"
      ],
      "metadata": {
        "id": "d0OO-HEURn_m"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 13: Initialize Evaluation Counters\n",
        "\n",
        "These counters will be used to track the attack’s success and measure Top-1 and Top-5 accuracy on adversarial images."
      ],
      "metadata": {
        "id": "KW-4gGxxbm8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counters for statistics and accuracy tracking\n",
        "total_images = 0\n",
        "successful_attacks = 0            # When prediction changes\n",
        "correct_original = 0              # Original prediction correct\n",
        "correct_adversarial_top1 = 0      # Top-1 correct on perturbed image\n",
        "correct_adversarial_top5 = 0      # Top-5 correct on perturbed image\n",
        "max_l_inf = 0                     # Max L∞ distance observed\n",
        "avg_l_inf = 0                     # Average L∞ distance (sum, to be divided later)\n"
      ],
      "metadata": {
        "id": "jFi9B2sjRqaT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 14: Prepare a List for Visualization\n",
        "\n",
        "This will store up to 5 notable attack examples for visual inspection and reporting."
      ],
      "metadata": {
        "id": "tpWKux67bsWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store visualization examples\n",
        "examples_to_visualize = []"
      ],
      "metadata": {
        "id": "9LV861LcRst0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 15: Perform FGSM on All Test Images\n",
        "\n",
        "This loop goes through every image, computes gradients, generates adversarial versions, saves them, evaluates the model on them, and tracks results."
      ],
      "metadata": {
        "id": "aT73Kylebyew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform FGSM attack on each image\n",
        "print(\"Starting FGSM attacks...\")\n",
        "for i, (image, label, folder, img_name) in enumerate(attack_loader):\n",
        "    # Send data to device\n",
        "    image, label = image.to(device), label.to(device)\n",
        "\n",
        "    # Set requires_grad attribute of tensor\n",
        "    image.requires_grad = True\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(image)\n",
        "\n",
        "    # Get original prediction\n",
        "    _, original_pred = torch.max(output, 1)\n",
        "    original_correct = (original_pred == label).item()\n",
        "    if original_correct:\n",
        "        correct_original += 1\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.cross_entropy(output, label)\n",
        "\n",
        "    # Backward pass to get gradients\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Get gradients of the input image\n",
        "    data_grad = image.grad.data\n",
        "\n",
        "    # Call FGSM attack\n",
        "    perturbed_image = fgsm_attack(image, epsilon, data_grad)\n",
        "\n",
        "    # Calculate L∞ distance\n",
        "    l_inf = l_inf_distance(image, perturbed_image)\n",
        "    max_l_inf = max(max_l_inf, l_inf)\n",
        "    avg_l_inf += l_inf\n",
        "\n",
        "    # Forward pass with perturbed image\n",
        "    with torch.no_grad():\n",
        "        output_adv = model(perturbed_image)\n",
        "\n",
        "    # Get adversarial prediction\n",
        "    _, adv_pred = torch.max(output_adv, 1)\n",
        "\n",
        "    # Check if attack was successful in changing prediction\n",
        "    attack_success = (original_pred != adv_pred).item()\n",
        "    if attack_success:\n",
        "        successful_attacks += 1\n",
        "\n",
        "    # Check if adversarial example is still classified correctly (top-1)\n",
        "    adversarial_correct_top1 = (adv_pred == label).item()\n",
        "    if adversarial_correct_top1:\n",
        "        correct_adversarial_top1 += 1\n",
        "\n",
        "    # Check top-5 accuracy\n",
        "    _, top5_indices = output_adv.topk(5, dim=1)\n",
        "    adversarial_correct_top5 = label.item() in top5_indices.squeeze().tolist()\n",
        "    if adversarial_correct_top5:\n",
        "        correct_adversarial_top5 += 1\n",
        "\n",
        "    # Save the adversarial image\n",
        "    save_path = os.path.join(SAVE_PATH, folder[0], img_name[0])\n",
        "    adv_img_to_save = perturbed_image.detach().cpu().squeeze(0)\n",
        "    save_normalized_image(adv_img_to_save, save_path, mean_norms, std_norms)\n",
        "\n",
        "    # Collect examples for visualization (original correct but adversarial wrong)\n",
        "    if original_correct and not adversarial_correct_top1 and len(examples_to_visualize) < 5:\n",
        "        examples_to_visualize.append({\n",
        "            'original': image.detach(),\n",
        "            'adversarial': perturbed_image.detach(),\n",
        "            'original_label': original_pred.item(),\n",
        "            'adversarial_label': adv_pred.item(),\n",
        "            'true_label': label.item(),\n",
        "            'index': len(examples_to_visualize) + 1\n",
        "        })\n",
        "\n",
        "    # Update counter\n",
        "    total_images += 1\n",
        "\n",
        "    # Print progress\n",
        "    if (i + 1) % 50 == 0:\n",
        "        print(f\"Processed {i + 1}/{len(attack_loader)} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COU91RWVTdCS",
        "outputId": "14a7375e-f233-4bac-c76b-90c23ccc179b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting FGSM attacks...\n",
            "Processed 50/500 images\n",
            "Processed 100/500 images\n",
            "Processed 150/500 images\n",
            "Processed 200/500 images\n",
            "Processed 250/500 images\n",
            "Processed 300/500 images\n",
            "Processed 350/500 images\n",
            "Processed 400/500 images\n",
            "Processed 450/500 images\n",
            "Processed 500/500 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 16: Compute Final Accuracy and Statistics\n",
        "\n",
        "This block calculates key metrics such as:\n",
        "\n",
        "Top-1 and Top-5 accuracy after the attack\n",
        "\n",
        "Success rate of FGSM (how often prediction changed)\n",
        "\n",
        "Average and maximum L∞ distortion\n",
        "\n",
        "Percentage drop in accuracy"
      ],
      "metadata": {
        "id": "kD6t3-r7b6cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute key metrics for evaluating FGSM attack effectiveness\n",
        "avg_l_inf = avg_l_inf / total_images                         # Average L∞ distance\n",
        "attack_success_rate = 100 * successful_attacks / total_images  # % of inputs where prediction changed\n",
        "\n",
        "# Accuracy before and after attack\n",
        "original_acc_top1 = 100 * correct_original / total_images\n",
        "adversarial_acc_top1 = 100 * correct_adversarial_top1 / total_images\n",
        "adversarial_acc_top5 = 100 * correct_adversarial_top5 / total_images\n",
        "\n",
        "# Drop in Top-1 accuracy\n",
        "relative_drop = 100 * (original_acc_top1 - adversarial_acc_top1) / original_acc_top1\n"
      ],
      "metadata": {
        "id": "Cr3YeTd8Tj5p"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 17: Print the FGSM Evaluation Results\n",
        "\n",
        "Here we print a summary of the attack performance, including distortion statistics, success rate, and accuracy drops."
      ],
      "metadata": {
        "id": "wyiDevIjcC99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results\n",
        "print(f\"\\nFGSM Attack Results (ε = {epsilon}):\")\n",
        "print(f\"Maximum L∞ distance: {max_l_inf:.6f}\")\n",
        "print(f\"Average L∞ distance: {avg_l_inf:.6f}\")\n",
        "print(f\"Attack success rate: {attack_success_rate:.2f}%\")\n",
        "print(\"\\nModel Accuracy:\")\n",
        "print(f\"Original Top-1: {original_acc_top1:.2f}%\")\n",
        "print(f\"Adversarial Top-1: {adversarial_acc_top1:.2f}%\")\n",
        "print(f\"Adversarial Top-5: {adversarial_acc_top5:.2f}%\")\n",
        "print(f\"Relative accuracy drop: {relative_drop:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7ekNEkJTmoE",
        "outputId": "7380bbf1-f0d4-4316-9ff7-d118071aac70"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FGSM Attack Results (ε = 0.02):\n",
            "Maximum L∞ distance: 0.020000\n",
            "Average L∞ distance: 0.020000\n",
            "Attack success rate: 76.20%\n",
            "\n",
            "Model Accuracy:\n",
            "Original Top-1: 70.60%\n",
            "Adversarial Top-1: 5.00%\n",
            "Adversarial Top-5: 29.40%\n",
            "Relative accuracy drop: 92.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 18: Check Against Project Goal\n",
        "\n",
        "This cell checks whether the attack reduced accuracy by at least 50%, as required in the project specification."
      ],
      "metadata": {
        "id": "KfLmza9KcH-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if relative_drop >= 50:\n",
        "    print(\"\\nSuccess! Achieved more than 50% accuracy drop!\")\n",
        "else:\n",
        "    print(f\"\\nTarget not met. Need 50% drop, got {relative_drop:.2f}% drop.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHnNptnfTq4u",
        "outputId": "39e18a4f-0a5f-4232-eba7-b894cb48f43b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Success! Achieved more than 50% accuracy drop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 19: Generate Visualizations of Attack Results\n",
        "\n",
        "Here we visualize 3–5 examples where the attack fooled the model — by showing the original, adversarial, and the difference image."
      ],
      "metadata": {
        "id": "hFunrit3cLO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize examples\n",
        "print(\"\\nVisualizing attack examples...\")\n",
        "for example in examples_to_visualize:\n",
        "    original_class = get_class_name(example['true_label'])\n",
        "    adversarial_class = get_class_name(example['adversarial_label'])\n",
        "\n",
        "    visualize_attack(\n",
        "        example['original'],\n",
        "        example['adversarial'],\n",
        "        example['original_label'],\n",
        "        example['adversarial_label'],\n",
        "        original_class,\n",
        "        adversarial_class,\n",
        "        example['index']\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd0ZYkxkTtdk",
        "outputId": "2c191e41-481f-4efa-d9ba-d6d483a58337"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing attack examples...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 20: Print Final Log and Save Confirmation\n",
        "\n",
        "This logs how many examples were visualized and where the adversarial image dataset was saved."
      ],
      "metadata": {
        "id": "eaqTpf-ZcVLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Saved {len(examples_to_visualize)} visualization examples\")\n",
        "print(f\"All adversarial images saved to {SAVE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvgYJMQCU8Fm",
        "outputId": "c4fec0a5-2f04-4798-cfc1-a73d337bb000"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 5 visualization examples\n",
            "All adversarial images saved to ./AdversarialTestSet1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 21: Create Combined Visualization Grid\n",
        "\n",
        "This function produces a single image grid showing all selected adversarial examples side-by-side with their original and difference images. It’s useful for summarizing attack effectiveness in your final report."
      ],
      "metadata": {
        "id": "agDLit6Hcd4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a grid-style visualization of multiple adversarial examples\n",
        "def create_combined_visualization(examples_to_visualize):\n",
        "    if not examples_to_visualize:\n",
        "        print(\"No examples to visualize\")\n",
        "        return\n",
        "\n",
        "    # Set up a tall figure: 1 row per example, 3 columns (original, adversarial, difference)\n",
        "    plt.figure(figsize=(15, 5 * len(examples_to_visualize)))\n",
        "\n",
        "    for i, example in enumerate(examples_to_visualize):\n",
        "        original_class = get_class_name(example['true_label'])\n",
        "        adversarial_class = get_class_name(example['adversarial_label'])\n",
        "\n",
        "        # -- Original Image --\n",
        "        plt.subplot(len(examples_to_visualize), 3, i * 3 + 1)\n",
        "        img = example['original'].cpu().squeeze()\n",
        "        img_display = img.clone()\n",
        "        for t, m, s in zip(img_display, mean_norms, std_norms):\n",
        "            t.mul_(s).add_(m)\n",
        "        img_display = img_display.permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "        plt.imshow(img_display)\n",
        "        plt.title(f\"Original: {original_class}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # -- Adversarial Image --\n",
        "        plt.subplot(len(examples_to_visualize), 3, i * 3 + 2)\n",
        "        adv_img = example['adversarial'].cpu().squeeze()\n",
        "        adv_img_display = adv_img.clone()\n",
        "        for t, m, s in zip(adv_img_display, mean_norms, std_norms):\n",
        "            t.mul_(s).add_(m)\n",
        "        adv_img_display = adv_img_display.permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "        plt.imshow(adv_img_display)\n",
        "        plt.title(f\"Adversarial: {adversarial_class}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # -- Difference Image (magnified for visibility) --\n",
        "        plt.subplot(len(examples_to_visualize), 3, i * 3 + 3)\n",
        "        diff = np.abs(adv_img_display - img_display) * 10  # Amplify difference\n",
        "        plt.imshow(diff)\n",
        "        plt.title(\"Difference (x10)\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    # Save the final combined visualization to file\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('fgsm_attack_examples.png', dpi=300)\n",
        "    plt.close()\n",
        "    print(\"Combined visualization saved to 'fgsm_attack_examples.png'\")\n"
      ],
      "metadata": {
        "id": "pcXWy0nLU-ee"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 22:Individual Visualizations for Report\n",
        "\n",
        "This block reuses the visualize_attack function to generate and save individual side-by-side comparisons of original vs adversarial examples. This might be a re-run for confirming outputs"
      ],
      "metadata": {
        "id": "3KYyzNxpcq58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage with the examples collected:\n",
        "print(\"\\nVisualizing attack examples...\")\n",
        "for example in examples_to_visualize:\n",
        "    original_class = get_class_name(example['true_label'])\n",
        "    adversarial_class = get_class_name(example['adversarial_label'])\n",
        "\n",
        "    visualize_attack(\n",
        "        example['original'],\n",
        "        example['adversarial'],\n",
        "        example['original_label'],\n",
        "        example['adversarial_label'],\n",
        "        original_class,\n",
        "        adversarial_class,\n",
        "        example['index']\n",
        "    )\n",
        "\n",
        "print(f\"Saved {len(examples_to_visualize)} visualization examples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwtptGrpVGvU",
        "outputId": "4e1fd83c-9e1a-40c7-d9e5-0b4c448e0c06"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing attack examples...\n",
            "Saved 5 visualization examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a combined visualization\n",
        "if examples_to_visualize:\n",
        "    create_combined_visualization(examples_to_visualize)\n",
        "\n",
        "# Save the mapping and labels to the adversarial dataset folder\n",
        "with open(os.path.join(SAVE_PATH, 'imagenet_mapping.json'), 'w') as f:\n",
        "    json.dump(mapping_dict, f, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tWNNpHeELSw",
        "outputId": "8835e47f-5252-457e-85be-fc03d3e77736"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined visualization saved to 'fgsm_attack_examples.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Improved Adversarial Attacks — PGD (Projected Gradient Descent)\n",
        "\n",
        "Task Description\n",
        "\n",
        "The goal of this task is to build a stronger attack than FGSM. While FGSM applies a single-step perturbation, this task explores multi-step methods like Projected Gradient Descent (PGD), which repeatedly refines the perturbation to further degrade model accuracy while still adhering to an ε-bound under L∞.\n",
        "We aim to:\n",
        "\n",
        "Implement PGD with multiple gradient steps.\n",
        "\n",
        "Maintain the L∞ constraint strictly.\n",
        "\n",
        "Visualize and evaluate performance using the same metrics as Task 2.\n",
        "\n",
        "Achieve a >70% accuracy drop relative to Task 1's clean accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "ENDgeesCeLjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Prepare Directory for Adversarial Test Set 2\n",
        "\n",
        "This creates a folder to store adversarial samples generated using PGD.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tbmeKi3ueSW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory to save the adversarial images\n",
        "SAVE_PATH = \"./AdversarialTestSet2\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)"
      ],
      "metadata": {
        "id": "W9eQJRqXZOJ6"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Replicate Folder Structure from Original Dataset\n",
        "\n",
        "We mirror the original class-wise folder structure to organize PGD samples."
      ],
      "metadata": {
        "id": "xf-g6GPreaof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy folder structure from original dataset\n",
        "for folder in folders:\n",
        "    os.makedirs(os.path.join(SAVE_PATH, folder), exist_ok=True)"
      ],
      "metadata": {
        "id": "7fyd1z9PZYPg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 3: Define Helper – Save Normalized Image\n",
        "\n",
        "This function converts a normalized tensor back to a displayable image and saves it to disk."
      ],
      "metadata": {
        "id": "HE0LEGYhedd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrected helper function to denormalize and save images\n",
        "def save_normalized_image(tensor, path, mean, std):\n",
        "    # Create a copy of the tensor\n",
        "    img = tensor.clone()\n",
        "\n",
        "    # Unnormalize the image\n",
        "    for t, m, s in zip(img, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "\n",
        "    # Ensure the pixel values are in [0, 1]\n",
        "    img = torch.clamp(img, 0, 1)\n",
        "\n",
        "    # Save the image\n",
        "    vutils.save_image(img, path)\n"
      ],
      "metadata": {
        "id": "pyyqnpBrZcOK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 4: Define Helper – L∞ Distance\n",
        "\n",
        "Used to compute the maximum pixel difference between original and perturbed images."
      ],
      "metadata": {
        "id": "pEVSeQ6Xelv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Corrected function to compute L∞ distance\n",
        "def l_inf_distance(original, perturbed):\n",
        "    # Calculate the maximum absolute difference\n",
        "    return (original - perturbed).abs().max().item()"
      ],
      "metadata": {
        "id": "Yn_Ge8AIZe5Q"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Implement PGD Attack (with L∞ Constraint)\n",
        "\n",
        "This is the core of Task 3 — a looped, iterative attack that applies multiple small perturbations and projects the result back into the ε-ball."
      ],
      "metadata": {
        "id": "aYl0vWN5ephJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved PGD Attack with strict constraint enforcement\n",
        "def pgd_attack(model, image, label, epsilon, alpha, num_iter, random_start=True):\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent Attack with strict constraint enforcement\n",
        "\n",
        "    Args:\n",
        "        model: The model to attack\n",
        "        image: The original image\n",
        "        label: The true label\n",
        "        epsilon: Maximum perturbation (L∞ norm constraint)\n",
        "        alpha: Step size for each iteration\n",
        "        num_iter: Number of iterations\n",
        "        random_start: Whether to start with a random perturbation\n",
        "\n",
        "    Returns:\n",
        "        perturbed_image: The adversarial example\n",
        "    \"\"\"\n",
        "    # Clone the image to avoid modifying the original\n",
        "    perturbed_image = image.clone().detach()\n",
        "\n",
        "    # Start with a random perturbation if specified\n",
        "    if random_start:\n",
        "        # Random perturbation within epsilon ball\n",
        "        random_noise = torch.FloatTensor(perturbed_image.shape).uniform_(-epsilon, epsilon).to(device)\n",
        "        perturbed_image = perturbed_image + random_noise\n",
        "\n",
        "        # Strictly enforce the epsilon constraint\n",
        "        delta = perturbed_image - image\n",
        "        if delta.abs().max() > epsilon:\n",
        "            delta = delta * (epsilon / delta.abs().max())\n",
        "            perturbed_image = image + delta\n",
        "\n",
        "    for i in range(num_iter):\n",
        "        # Set requires_grad attribute\n",
        "        perturbed_image.requires_grad = True\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(perturbed_image)\n",
        "\n",
        "        # Calculate loss (untargeted attack - maximize loss to true label)\n",
        "        model.zero_grad()\n",
        "        loss = F.cross_entropy(outputs, label)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Get gradient sign\n",
        "        grad_sign = perturbed_image.grad.data.sign()\n",
        "\n",
        "        # Update perturbed image\n",
        "        with torch.no_grad():\n",
        "            # Take a step in the gradient direction\n",
        "            perturbed_image = perturbed_image + alpha * grad_sign\n",
        "\n",
        "            # Calculate and enforce the epsilon constraint exactly\n",
        "            delta = perturbed_image - image\n",
        "            if delta.abs().max() > epsilon:\n",
        "                delta = delta * (epsilon / delta.abs().max())\n",
        "\n",
        "            # Apply the constrained delta\n",
        "            perturbed_image = image + delta\n",
        "\n",
        "    # Final check to ensure constraint is met\n",
        "    delta = perturbed_image - image\n",
        "    max_delta = delta.abs().max().item()\n",
        "    if max_delta > epsilon:\n",
        "        # Apply strict scaling\n",
        "        scale_factor = epsilon / max_delta\n",
        "        delta = delta * scale_factor\n",
        "        perturbed_image = image + delta\n",
        "\n",
        "    # Verify constraint\n",
        "    final_linf = l_inf_distance(image, perturbed_image)\n",
        "    assert final_linf <= epsilon + 1e-5, f\"Constraint violation: {final_linf} > {epsilon}\"\n",
        "\n",
        "    return perturbed_image"
      ],
      "metadata": {
        "id": "ISHMMw7XZiWW"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Define Visualization for PGD Examples\n",
        "\n",
        "This is the same structure as FGSM, adapted for PGD examples."
      ],
      "metadata": {
        "id": "ZSQEEt-yevmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize attack examples\n",
        "def visualize_attack(original, adversarial, original_label, adversarial_label,\n",
        "                     original_class, adversarial_class, index):\n",
        "    \"\"\"\n",
        "    Visualize the original and adversarial images side by side\n",
        "    along with their predictions\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Display original image\n",
        "    plt.subplot(1, 3, 1)\n",
        "    img = original.cpu().squeeze()  # Shape [3, 224, 224]\n",
        "\n",
        "    # Unnormalize\n",
        "    img_display = img.clone()\n",
        "    for t, m, s in zip(img_display, mean_norms, std_norms):\n",
        "        t.mul_(s).add_(m)\n",
        "\n",
        "    # Properly permute for display\n",
        "    img_display = img_display.permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "    plt.imshow(img_display)\n",
        "    plt.title(f\"Original\\nLabel: {original_label}\\nClass: {original_class}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Display adversarial image\n",
        "    plt.subplot(1, 3, 2)\n",
        "    adv_img = adversarial.cpu().squeeze()  # Shape [3, 224, 224]\n",
        "\n",
        "    # Unnormalize\n",
        "    adv_img_display = adv_img.clone()\n",
        "    for t, m, s in zip(adv_img_display, mean_norms, std_norms):\n",
        "        t.mul_(s).add_(m)\n",
        "\n",
        "    # Properly permute for display\n",
        "    adv_img_display = adv_img_display.permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "    plt.imshow(adv_img_display)\n",
        "    plt.title(f\"Adversarial\\nLabel: {adversarial_label}\\nClass: {adversarial_class}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Display the difference (magnified for visibility)\n",
        "    plt.subplot(1, 3, 3)\n",
        "    diff = np.abs(adv_img_display - img_display) * 10  # Magnify by 10x to make differences visible\n",
        "    plt.imshow(diff)\n",
        "    plt.title(\"Difference (x10)\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'pgd_attack_example_{index}.png')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "8IyJCUi3ZnnV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Combined Visualization of PGD Results\n",
        "\n",
        "Creates a full grid of PGD adversarial examples for reporting."
      ],
      "metadata": {
        "id": "M9IPMqKTeyxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a combined visualization of examples\n",
        "def create_combined_visualization(examples_to_visualize):\n",
        "    if not examples_to_visualize:\n",
        "        print(\"No examples to visualize\")\n",
        "        return\n",
        "\n",
        "    # Create a figure with 3 columns (original, adversarial, difference) and one row per example\n",
        "    plt.figure(figsize=(15, 5 * len(examples_to_visualize)))\n",
        "\n",
        "    for i, example in enumerate(examples_to_visualize):\n",
        "        original_class = get_class_name(example['true_label'])\n",
        "        adversarial_class = get_class_name(example['adversarial_label'])\n",
        "\n",
        "        # Original image\n",
        "        plt.subplot(len(examples_to_visualize), 3, i*3 + 1)\n",
        "        img = example['original'].cpu().squeeze()  # Shape [3, 224, 224]\n",
        "\n",
        "        # Unnormalize\n",
        "        img_display = img.clone()\n",
        "        for t, m, s in zip(img_display, mean_norms, std_norms):\n",
        "            t.mul_(s).add_(m)\n",
        "\n",
        "        # Permute correctly\n",
        "        img_display = img_display.permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "        plt.imshow(img_display)\n",
        "        plt.title(f\"Original: {original_class}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Adversarial image\n",
        "        plt.subplot(len(examples_to_visualize), 3, i*3 + 2)\n",
        "        adv_img = example['adversarial'].cpu().squeeze()  # Shape [3, 224, 224]\n",
        "\n",
        "        # Unnormalize\n",
        "        adv_img_display = adv_img.clone()\n",
        "        for t, m, s in zip(adv_img_display, mean_norms, std_norms):\n",
        "            t.mul_(s).add_(m)\n",
        "\n",
        "        # Permute correctly\n",
        "        adv_img_display = adv_img_display.permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "        plt.imshow(adv_img_display)\n",
        "        plt.title(f\"Adversarial: {adversarial_class}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Difference (magnified)\n",
        "        plt.subplot(len(examples_to_visualize), 3, i*3 + 3)\n",
        "        diff = np.abs(adv_img_display - img_display) * 10  # Magnify by 10x\n",
        "        plt.imshow(diff)\n",
        "        plt.title(\"Difference (x10)\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('pgd_attack_examples.png', dpi=300)\n",
        "    plt.close()\n",
        "    print(\"Combined visualization saved to 'pgd_attack_examples.png'\")"
      ],
      "metadata": {
        "id": "cJWHkpNrZqFK"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 8: Prepare DataLoader for PGD\n",
        "\n",
        "Just like FGSM, we process one image at a time, so we use a batch size of 1."
      ],
      "metadata": {
        "id": "B9K7ou0oe4bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get class name from label\n",
        "def get_class_name(label):\n",
        "    for folder, l in folder_to_label.items():\n",
        "        if l == label:\n",
        "            return mapping_dict['folder_to_name'].get(folder, str(label))\n",
        "    return str(label)"
      ],
      "metadata": {
        "id": "Qsg0qFW3ZtOO"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Define a Custom Dataset Class for PGD\n",
        "\n",
        "We define FGSMDataset again to retrieve not only image tensors and labels, but also metadata like the folder and filename. This is useful for saving adversarial images in the correct locations."
      ],
      "metadata": {
        "id": "-6H4huiAfKc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified dataset class for loading images\n",
        "class FGSMDataset(Dataset):\n",
        "    def __init__(self, root_dir, folder_to_label, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.folder_to_label = folder_to_label\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect all image paths and their labels\n",
        "        self.samples = []\n",
        "        for folder, label in folder_to_label.items():\n",
        "            folder_path = os.path.join(root_dir, folder)\n",
        "            if os.path.isdir(folder_path):\n",
        "                for img_name in os.listdir(folder_path):\n",
        "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        img_path = os.path.join(folder_path, img_name)\n",
        "                        # Store folder name and file name for saving later\n",
        "                        self.samples.append((img_path, label, folder, img_name))\n",
        "\n",
        "        print(f\"Loaded {len(self.samples)} images from {len(folder_to_label)} class folders\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label, folder, img_name = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label, folder, img_name"
      ],
      "metadata": {
        "id": "PJ7RcfgaZvr-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Create the Dataset for PGD\n",
        "\n",
        "We now create an instance of this dataset using the dataset folder and transform pipeline."
      ],
      "metadata": {
        "id": "80RwoGuCe-fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset for the attack\n",
        "attack_dataset = FGSMDataset(\n",
        "    root_dir=dataset_path,\n",
        "    folder_to_label=folder_to_label,\n",
        "    transform=plain_transforms\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGljkioxZyXW",
        "outputId": "faf11cc5-9d55-44d1-851f-8bd0c9a4ff60"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 500 images from 100 class folders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Load Dataset with Batch Size 1\n",
        "\n",
        "We use a batch size of 1 so that each image can be attacked and saved individually."
      ],
      "metadata": {
        "id": "g4GRLq4xfR22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use batch size of 1 to simplify attack implementation\n",
        "attack_loader = DataLoader(attack_dataset, batch_size=1, shuffle=False)\n"
      ],
      "metadata": {
        "id": "1RhZrSa2Z0sT"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 9: Set PGD Attack Parameters\n",
        "These hyperparameters define the strength and structure of the PGD attack:\n",
        "\n",
        "epsilon: maximum allowable pixel change (same as FGSM).\n",
        "\n",
        "alpha: step size in each iteration.\n",
        "\n",
        "num_iter: number of PGD steps.\n",
        "\n",
        "random_start: whether to begin from a random point within the ε-ball."
      ],
      "metadata": {
        "id": "Ocs9XyRKfY9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set attack parameters\n",
        "epsilon = 0.02       # Maximum perturbation (same as FGSM)\n",
        "alpha = 0.002        # Step size (smaller than epsilon)\n",
        "num_iter = 20        # Number of iterations (increased)\n",
        "random_start = True  # Use random initialization"
      ],
      "metadata": {
        "id": "ia1EzhAKZ3Iy"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 10: Initialize Counters for Evaluation\n",
        "\n",
        "These counters will help track model performance on clean and adversarial examples, as well as record distortion statistics."
      ],
      "metadata": {
        "id": "2gNzLyoEfbEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize counters for statistics\n",
        "total_images = 0\n",
        "successful_attacks = 0  # Where prediction changed\n",
        "correct_original = 0\n",
        "correct_adversarial_top1 = 0\n",
        "correct_adversarial_top5 = 0\n",
        "max_l_inf = 0\n",
        "avg_l_inf = 0"
      ],
      "metadata": {
        "id": "W4Do7q5lZ5UH"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store visualization examples\n",
        "examples_to_visualize = []"
      ],
      "metadata": {
        "id": "cFZAiQ5qZ7e6"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 12: Run PGD Attack on the Dataset\n",
        "\n",
        "This loop applies the PGD attack to each image, tracks success/failure, measures distortion, saves adversarial images, and stores examples for visualization."
      ],
      "metadata": {
        "id": "x5IqMP89ff74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform PGD attack on each image\n",
        "print(\"Starting PGD attacks...\")\n",
        "for i, (image, label, folder, img_name) in enumerate(attack_loader):\n",
        "    # Send data to device\n",
        "    image, label = image.to(device), label.to(device)\n",
        "\n",
        "    # Get original prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        _, original_pred = torch.max(output, 1)\n",
        "\n",
        "    original_correct = (original_pred == label).item()\n",
        "    if original_correct:\n",
        "        correct_original += 1\n",
        "\n",
        "    # Perform PGD attack\n",
        "    perturbed_image = pgd_attack(\n",
        "        model=model,\n",
        "        image=image,\n",
        "        label=label,\n",
        "        epsilon=epsilon,\n",
        "        alpha=alpha,\n",
        "        num_iter=num_iter,\n",
        "        random_start=random_start\n",
        "    )\n",
        "\n",
        "    # Calculate L∞ distance\n",
        "    l_inf = l_inf_distance(image, perturbed_image)\n",
        "    max_l_inf = max(max_l_inf, l_inf)\n",
        "    avg_l_inf += l_inf\n",
        "\n",
        "    # Forward pass with perturbed image\n",
        "    with torch.no_grad():\n",
        "        output_adv = model(perturbed_image)\n",
        "\n",
        "    # Get adversarial prediction\n",
        "    _, adv_pred = torch.max(output_adv, 1)\n",
        "\n",
        "    # Check if attack was successful in changing prediction\n",
        "    attack_success = (original_pred != adv_pred).item()\n",
        "    if attack_success:\n",
        "        successful_attacks += 1\n",
        "\n",
        "    # Check if adversarial example is still classified correctly (top-1)\n",
        "    adversarial_correct_top1 = (adv_pred == label).item()\n",
        "    if adversarial_correct_top1:\n",
        "        correct_adversarial_top1 += 1\n",
        "\n",
        "    # Check top-5 accuracy\n",
        "    _, top5_indices = output_adv.topk(5, dim=1)\n",
        "    adversarial_correct_top5 = label.item() in top5_indices.squeeze().tolist()\n",
        "    if adversarial_correct_top5:\n",
        "        correct_adversarial_top5 += 1\n",
        "\n",
        "    # Save the adversarial image\n",
        "    save_path = os.path.join(SAVE_PATH, folder[0], img_name[0])\n",
        "    adv_img_to_save = perturbed_image.detach().cpu().squeeze(0)\n",
        "    save_normalized_image(adv_img_to_save, save_path, mean_norms, std_norms)\n",
        "\n",
        "    # Collect examples for visualization (original correct but adversarial wrong)\n",
        "    if original_correct and not adversarial_correct_top1 and len(examples_to_visualize) < 5:\n",
        "        examples_to_visualize.append({\n",
        "            'original': image.detach(),\n",
        "            'adversarial': perturbed_image.detach(),\n",
        "            'original_label': original_pred.item(),\n",
        "            'adversarial_label': adv_pred.item(),\n",
        "            'true_label': label.item(),\n",
        "            'index': len(examples_to_visualize) + 1\n",
        "        })\n",
        "\n",
        "    # Update counter\n",
        "    total_images += 1\n",
        "\n",
        "    # Print progress\n",
        "    if (i + 1) % 50 == 0:\n",
        "        print(f\"Processed {i + 1}/{len(attack_loader)} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ueha0eL4Z-Sl",
        "outputId": "28eab1e4-7338-45db-e64e-f8a32932da2b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting PGD attacks...\n",
            "Processed 50/500 images\n",
            "Processed 100/500 images\n",
            "Processed 150/500 images\n",
            "Processed 200/500 images\n",
            "Processed 250/500 images\n",
            "Processed 300/500 images\n",
            "Processed 350/500 images\n",
            "Processed 400/500 images\n",
            "Processed 450/500 images\n",
            "Processed 500/500 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 13: Compute PGD Evaluation Statistics\n",
        "\n",
        "After looping through all images, we compute final statistics like distortion, accuracy, and relative accuracy drop."
      ],
      "metadata": {
        "id": "xuoGXhtGflPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate final statistics\n",
        "avg_l_inf = avg_l_inf / total_images\n",
        "attack_success_rate = 100 * successful_attacks / total_images\n",
        "original_acc_top1 = 100 * correct_original / total_images\n",
        "adversarial_acc_top1 = 100 * correct_adversarial_top1 / total_images\n",
        "adversarial_acc_top5 = 100 * correct_adversarial_top5 / total_images\n",
        "relative_drop = 100 * (original_acc_top1 - adversarial_acc_top1) / original_acc_top1\n"
      ],
      "metadata": {
        "id": "IzeoVqHJakt5"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 14: Print Final Results for PGD\n",
        "\n",
        "Print the success of the attack including accuracy degradation, distortion, and success rate.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N2qoVIrogLah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results\n",
        "print(f\"\\nPGD Attack Results (ε = {epsilon}, α = {alpha}, steps = {num_iter}):\")\n",
        "print(f\"Maximum L∞ distance: {max_l_inf:.6f}\")\n",
        "print(f\"Average L∞ distance: {avg_l_inf:.6f}\")\n",
        "print(f\"Attack success rate: {attack_success_rate:.2f}%\")\n",
        "print(\"\\nModel Accuracy:\")\n",
        "print(f\"Original Top-1: {original_acc_top1:.2f}%\")\n",
        "print(f\"Adversarial Top-1: {adversarial_acc_top1:.2f}%\")\n",
        "print(f\"Adversarial Top-5: {adversarial_acc_top5:.2f}%\")\n",
        "print(f\"Relative accuracy drop: {relative_drop:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXPupdjuanKK",
        "outputId": "fc7a629f-b1d3-42c6-9492-c34aa26633da"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PGD Attack Results (ε = 0.02, α = 0.002, steps = 20):\n",
            "Maximum L∞ distance: 0.020000\n",
            "Average L∞ distance: 0.020000\n",
            "Attack success rate: 82.00%\n",
            "\n",
            "Model Accuracy:\n",
            "Original Top-1: 70.60%\n",
            "Adversarial Top-1: 0.40%\n",
            "Adversarial Top-5: 12.00%\n",
            "Relative accuracy drop: 99.43%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if relative_drop >= 70:\n",
        "    print(\"\\nSuccess! Achieved more than 70% accuracy drop!\")\n",
        "else:\n",
        "    print(f\"\\nTarget not met. Need 70% drop, got {relative_drop:.2f}% drop.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xZdBg4oasH4",
        "outputId": "b51f6d07-9669-417a-8468-13ca7926139d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Success! Achieved more than 70% accuracy drop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 15: Visualize PGD Attack Examples Individually\n",
        "\n",
        "This block loops through a few handpicked adversarial examples (stored earlier), and uses visualize_attack() to save individual comparison images."
      ],
      "metadata": {
        "id": "BFuORwm6gT8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize examples\n",
        "print(\"\\nVisualizing attack examples...\")\n",
        "for example in examples_to_visualize:\n",
        "    original_class = get_class_name(example['true_label'])\n",
        "    adversarial_class = get_class_name(example['adversarial_label'])\n",
        "\n",
        "    visualize_attack(\n",
        "        example['original'],\n",
        "        example['adversarial'],\n",
        "        example['original_label'],\n",
        "        example['adversarial_label'],\n",
        "        original_class,\n",
        "        adversarial_class,\n",
        "        example['index']\n",
        "    )\n",
        "\n",
        "print(f\"Saved {len(examples_to_visualize)} visualization examples\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv3Wfkb0a9id",
        "outputId": "a2ca12dd-49d7-4cef-ba9f-0b76e9ade8ef"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing attack examples...\n",
            "Saved 5 visualization examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 16: Create Combined Visualization Grid\n",
        "\n",
        "This function generates a grid image showing all examples side by side — great for including in your final report or presentation."
      ],
      "metadata": {
        "id": "G2ab0G2TgXjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a combined visualization\n",
        "if examples_to_visualize:\n",
        "    create_combined_visualization(examples_to_visualize)\n",
        "\n",
        "# Save the mapping and labels to the adversarial dataset folder\n",
        "with open(os.path.join(SAVE_PATH, 'imagenet_mapping2.json'), 'w') as f:\n",
        "    json.dump(mapping_dict, f, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_2lcd_WbBf0",
        "outputId": "eb9e7f44-a952-47b0-fb29-4242b4f29a12"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined visualization saved to 'pgd_attack_examples.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torchvision.utils as vutils\n",
        "import random"
      ],
      "metadata": {
        "id": "6bbF9MSAheOD"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: Localized Patch Attacks (Targeted and Optimized)\n",
        "\n",
        "Task Description\n",
        "\n",
        "In this task, we design localized adversarial attacks by modifying only a small patch of the image. Unlike global attacks like FGSM/PGD, patch attacks aim to be less perceptible by targeting a square region of the image. The goal is to:\n",
        "\n",
        "Apply perturbation only in a localized square area (e.g., 32×32 pixels),\n",
        "\n",
        "Choose the most sensitive area of the image (highest gradient),\n",
        "\n",
        "Perform targeted misclassification, pushing predictions toward specific classes.\n",
        "\n",
        "We aim to generate:\n",
        "\n",
        "Successful adversarial samples within patch bounds,\n",
        "\n",
        "Visualizations of perturbed patches,\n",
        "\n",
        "A new dataset AdversarialTestSet3"
      ],
      "metadata": {
        "id": "lgeyiqL5grfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory to save the adversarial images\n",
        "SAVE_PATH = \"./AdversarialTestSet3\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)"
      ],
      "metadata": {
        "id": "zrLa3D6Lk1ch"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Replicate Folder Structure for Adversarial Output\n",
        "\n",
        "We mirror the original folder structure (one per class) to save patch-attacked images in an organized way.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x4jMZE3Ugz1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy folder structure from original dataset\n",
        "for folder in folders:\n",
        "    os.makedirs(os.path.join(SAVE_PATH, folder), exist_ok=True)"
      ],
      "metadata": {
        "id": "HhgUIO8nk4kD"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to denormalize and save images\n",
        "def save_normalized_image(tensor, path, mean, std):\n",
        "    # Create a copy of the tensor\n",
        "    img = tensor.clone()\n",
        "\n",
        "    # Unnormalize the image\n",
        "    for t, m, s in zip(img, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "\n",
        "    # Ensure the pixel values are in [0, 1]\n",
        "    img = torch.clamp(img, 0, 1)\n",
        "\n",
        "    # Save the image\n",
        "    vutils.save_image(img, path)"
      ],
      "metadata": {
        "id": "w3Svy432k7sS"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute L∞ distance (only in the patch region)\n",
        "def patch_l_inf_distance(original, perturbed, patch_mask):\n",
        "    # Calculate the maximum absolute difference only in the patch region\n",
        "    diff = (original - perturbed).abs() * patch_mask\n",
        "    return diff.max().item()"
      ],
      "metadata": {
        "id": "Och_4TLBk-gv"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Identify High-Gradient Patch Location\n",
        "\n",
        "This method finds the most sensitive 32×32 region in the image by computing the gradient of the loss w.r.t. the input pixels. The region with the highest accumulated gradient magnitude is selected as the target patch location."
      ],
      "metadata": {
        "id": "Jg56Csztg_Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the most important region to attack based on gradient magnitude\n",
        "def find_important_region(model, image, label, patch_size=32):\n",
        "    \"\"\"Locate the region with highest gradient energy\"\"\"\n",
        "    image.requires_grad = True\n",
        "    output = model(image)\n",
        "    loss = F.cross_entropy(output, label)\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Get gradient magnitude\n",
        "    grad_magnitude = image.grad.abs().sum(1).squeeze(0)  # Sum across channels\n",
        "\n",
        "    # Use a sliding window to find region with highest gradient energy\n",
        "    max_energy = -float('inf')\n",
        "    max_x, max_y = 0, 0\n",
        "\n",
        "    height, width = grad_magnitude.shape\n",
        "\n",
        "    for y in range(height - patch_size + 1):\n",
        "        for x in range(width - patch_size + 1):\n",
        "            window = grad_magnitude[y:y+patch_size, x:x+patch_size]\n",
        "            energy = window.sum().item()\n",
        "            if energy > max_energy:\n",
        "                max_energy = energy\n",
        "                max_x, max_y = x, y\n",
        "\n",
        "    return max_x, max_y"
      ],
      "metadata": {
        "id": "ckbGE-CqlBNp"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Optimized Patch Attack – Targeted & Adaptive\n",
        "\n",
        "This function:\n",
        "\n",
        "Finds the most sensitive region in the image to perturb,\n",
        "\n",
        "Selects a list of likely or effective target classes,\n",
        "\n",
        "Iteratively applies gradient updates only inside the patch to fool the model into predicting a specific target class,\n",
        "\n",
        "Falls back to high-contrast universal perturbation or best effort if no target succeeds.\n",
        "\n",
        "python\n",
        "Copy code\n"
      ],
      "metadata": {
        "id": "s9yF1TlNhDYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced single patch attack with optimized targeting and placement\n",
        "def optimized_patch_attack(model, image, label, patch_size=32, epsilon=1.0, alpha=0.1, num_iter=100):\n",
        "    \"\"\"\n",
        "    Enhanced single-patch attack that finds the most vulnerable location\n",
        "    and uses a highly-optimized targeted approach\n",
        "    \"\"\"\n",
        "    batch_size, channels, height, width = image.shape\n",
        "\n",
        "    # First, find the most important region to attack\n",
        "    with torch.enable_grad():\n",
        "        patch_x, patch_y = find_important_region(model, image.clone(), label, patch_size)\n",
        "\n",
        "    # Create a mask for the patch location\n",
        "    patch_mask = torch.zeros_like(image)\n",
        "    patch_mask[:, :, patch_y:patch_y+patch_size, patch_x:patch_x+patch_size] = 1.0\n",
        "\n",
        "    # Try multiple target classes\n",
        "    successful_perturbed = None\n",
        "    successful_target = None\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Get predictions for the original image\n",
        "    with torch.no_grad():\n",
        "        original_output = model(image)\n",
        "        _, original_pred = torch.max(original_output, 1)\n",
        "\n",
        "        # Find the most confusing classes (second highest prediction and others)\n",
        "        # These are good targets for the attack\n",
        "        probs = F.softmax(original_output, dim=1)\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, dim=1, descending=True)\n",
        "\n",
        "        # Create a prioritized list of targets:\n",
        "        # 1. Classes the model thinks are somewhat plausible (ranks 1-5)\n",
        "        # 2. Some fixed targets known to work well\n",
        "        # 3. Completely unrelated classes\n",
        "        target_candidates = []\n",
        "\n",
        "        # Add the top-5 predicted classes (except the true prediction)\n",
        "        for i in range(1, 5):  # Skip the top prediction (index 0)\n",
        "            if i < sorted_indices.shape[1]:\n",
        "                target_candidates.append(sorted_indices[0, i].item())\n",
        "\n",
        "        # Add some known effective targets\n",
        "        fixed_targets = [0, 150, 281, 389, 417, 504, 970]\n",
        "        target_candidates.extend(fixed_targets)\n",
        "\n",
        "        # Remove duplicates while preserving order\n",
        "        target_candidates = list(dict.fromkeys(target_candidates))\n",
        "\n",
        "        # Make sure the true label isn't a target\n",
        "        if label.item() in target_candidates:\n",
        "            target_candidates.remove(label.item())\n",
        "\n",
        "    # For each target class, try to generate an adversarial example\n",
        "    for target_idx in target_candidates[:10]:  # Try up to 10 targets\n",
        "        target_class = torch.tensor([target_idx], device=image.device)\n",
        "\n",
        "        # Initialize with some noise in the patch area\n",
        "        perturbed_image = image.clone().detach()\n",
        "        noise = torch.randn_like(image) * 0.1\n",
        "        perturbed_image = perturbed_image + noise * patch_mask\n",
        "\n",
        "        # Try a more direct adversarial pattern for this target\n",
        "        for i in range(num_iter):\n",
        "            perturbed_image.requires_grad = True\n",
        "            outputs = model(perturbed_image)\n",
        "\n",
        "            # Targeted attack: maximize target class, minimize true class\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Get logits for target and true class\n",
        "            target_logits = outputs[:, target_idx]\n",
        "            true_logits = outputs.gather(1, label.unsqueeze(1)).squeeze()\n",
        "\n",
        "            # Use a margin loss\n",
        "            loss = true_logits - target_logits + 10.0\n",
        "\n",
        "            # Check if we're already succeeding\n",
        "            with torch.no_grad():\n",
        "                _, pred = torch.max(outputs, 1)\n",
        "                if pred.item() == target_idx:\n",
        "                    successful_perturbed = perturbed_image.detach()\n",
        "                    successful_target = target_class\n",
        "                    break\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update perturbed image (only in patch area)\n",
        "            with torch.no_grad():\n",
        "                # Only update the patch area\n",
        "                grad = perturbed_image.grad.data\n",
        "                update = alpha * torch.sign(grad) * patch_mask\n",
        "                perturbed_image = perturbed_image - update  # Minimize loss\n",
        "\n",
        "                # Project perturbation back to epsilon ball (only in patch)\n",
        "                delta = (perturbed_image - image) * patch_mask\n",
        "                delta = torch.clamp(delta, -epsilon, epsilon)\n",
        "                perturbed_image = image + delta\n",
        "\n",
        "                # If this is our best attempt so far, save it\n",
        "                if loss.item() < best_loss:\n",
        "                    best_loss = loss.item()\n",
        "                    best_perturbed = perturbed_image.detach()\n",
        "\n",
        "        # If we found a successful attack with this target, return it\n",
        "        if successful_perturbed is not None:\n",
        "            return successful_perturbed, patch_mask\n",
        "\n",
        "    # If no target was completely successful, use a stronger approach\n",
        "    # with the most promising target (the one with lowest loss)\n",
        "\n",
        "    # If we have a \"best\" perturbed image, check if it actually works\n",
        "    if 'best_perturbed' in locals():\n",
        "        with torch.no_grad():\n",
        "            outputs = model(best_perturbed)\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            if pred.item() != label.item():\n",
        "                return best_perturbed, patch_mask\n",
        "\n",
        "    # If everything else failed, try a universal adversarial pattern\n",
        "    # Use a high-contrast pattern that often works well\n",
        "    universal_perturbed = image.clone()\n",
        "\n",
        "    # Create a checkerboard pattern\n",
        "    for c in range(channels):\n",
        "        for y in range(patch_size):\n",
        "            for x in range(patch_size):\n",
        "                if (x + y) % 2 == 0:\n",
        "                    universal_perturbed[:, c, patch_y+y, patch_x+x] = image[:, c, patch_y+y, patch_x+x] + epsilon\n",
        "                else:\n",
        "                    universal_perturbed[:, c, patch_y+y, patch_x+x] = image[:, c, patch_y+y, patch_x+x] - epsilon\n",
        "\n",
        "    # Ensure we stay within bounds in the patch area\n",
        "    delta = (universal_perturbed - image) * patch_mask\n",
        "    delta = torch.clamp(delta, -epsilon, epsilon)\n",
        "    universal_perturbed = image + delta\n",
        "\n",
        "    # Check if this works\n",
        "    with torch.no_grad():\n",
        "        outputs = model(universal_perturbed)\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        if pred.item() != label.item():\n",
        "            return universal_perturbed, patch_mask\n",
        "\n",
        "    # If all else fails, return our best attempt\n",
        "    if 'best_perturbed' in locals():\n",
        "        return best_perturbed, patch_mask\n",
        "    else:\n",
        "        return universal_perturbed, patch_mask\n"
      ],
      "metadata": {
        "id": "L_tq58bSlD61"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Visualize Individual Patch Attack Examples\n",
        "This function displays and saves four views for each example:\n",
        "\n",
        "Original image\n",
        "\n",
        "Adversarial image\n",
        "\n",
        "Patch mask\n",
        "\n",
        "Difference image (amplified)"
      ],
      "metadata": {
        "id": "i7e1pqlthNAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize attack examples\n",
        "def visualize_patch_attack(original, adversarial, patch_mask, original_label, adversarial_label,\n",
        "                     original_class, adversarial_class, index):\n",
        "    \"\"\"\n",
        "    Visualize the original, adversarial, and patch mask\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Display original image\n",
        "    plt.subplot(1, 4, 1)\n",
        "    img = original.cpu().squeeze()\n",
        "\n",
        "    # Unnormalize\n",
        "    img_display = img.clone()\n",
        "    for t, m, s in zip(img_display, mean_norms, std_norms):\n",
        "        t.mul_(s).add_(m)\n",
        "\n",
        "    # Properly permute for display\n",
        "    img_display = img_display.permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "    plt.imshow(img_display)\n",
        "    plt.title(f\"Original\\nLabel: {original_label}\\nClass: {original_class}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Display adversarial image\n",
        "    plt.subplot(1, 4, 2)\n",
        "    adv_img = adversarial.cpu().squeeze()\n",
        "\n",
        "    # Unnormalize\n",
        "    adv_img_display = adv_img.clone()\n",
        "    for t, m, s in zip(adv_img_display, mean_norms, std_norms):\n",
        "        t.mul_(s).add_(m)\n",
        "\n",
        "    # Properly permute for display\n",
        "    adv_img_display = adv_img_display.permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "    plt.imshow(adv_img_display)\n",
        "    plt.title(f\"Adversarial\\nLabel: {adversarial_label}\\nClass: {adversarial_class}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Display patch mask\n",
        "    plt.subplot(1, 4, 3)\n",
        "    mask = patch_mask.cpu().squeeze()[0]  # Take first channel of mask\n",
        "    plt.imshow(mask.numpy(), cmap='gray')\n",
        "    plt.title(\"Patch Location\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Display the difference (magnified for visibility)\n",
        "    plt.subplot(1, 4, 4)\n",
        "    diff = np.abs(adv_img_display - img_display) * 3  # Magnify by 3x\n",
        "    plt.imshow(diff)\n",
        "    plt.title(\"Difference (x3)\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'patch_attack_example_{index}.png')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "QexGzqyDlIZU"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a combined visualization\n",
        "def create_combined_patch_visualization(examples_to_visualize):\n",
        "    if not examples_to_visualize:\n",
        "        print(\"No examples to visualize\")\n",
        "        return\n",
        "\n",
        "    # Create a figure with 4 columns and one row per example\n",
        "    plt.figure(figsize=(20, 5 * len(examples_to_visualize)))\n",
        "\n",
        "    for i, example in enumerate(examples_to_visualize):\n",
        "        original_class = get_class_name(example['true_label'])\n",
        "        adversarial_class = get_class_name(example['adversarial_label'])\n",
        "\n",
        "        # Original image\n",
        "        plt.subplot(len(examples_to_visualize), 4, i*4 + 1)\n",
        "        img = example['original'].cpu().squeeze()\n",
        "\n",
        "        # Unnormalize\n",
        "        img_display = img.clone()\n",
        "        for t, m, s in zip(img_display, mean_norms, std_norms):\n",
        "            t.mul_(s).add_(m)\n",
        "\n",
        "        # Permute correctly\n",
        "        img_display = img_display.permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "        plt.imshow(img_display)\n",
        "        plt.title(f\"Original: {original_class}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Adversarial image\n",
        "        plt.subplot(len(examples_to_visualize), 4, i*4 + 2)\n",
        "        adv_img = example['adversarial'].cpu().squeeze()\n",
        "\n",
        "        # Unnormalize\n",
        "        adv_img_display = adv_img.clone()\n",
        "        for t, m, s in zip(adv_img_display, mean_norms, std_norms):\n",
        "            t.mul_(s).add_(m)\n",
        "\n",
        "        # Permute correctly\n",
        "        adv_img_display = adv_img_display.permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "        plt.imshow(adv_img_display)\n",
        "        plt.title(f\"Adversarial: {adversarial_class}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Patch mask\n",
        "        plt.subplot(len(examples_to_visualize), 4, i*4 + 3)\n",
        "        mask = example['patch_mask'].cpu().squeeze()[0]\n",
        "        plt.imshow(mask.numpy(), cmap='gray')\n",
        "        plt.title(\"Patch Location\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Difference (magnified)\n",
        "        plt.subplot(len(examples_to_visualize), 4, i*4 + 4)\n",
        "        diff = np.abs(adv_img_display - img_display) * 3  # Magnify by 3x\n",
        "        plt.imshow(diff)\n",
        "        plt.title(\"Difference (x3)\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('patch_attack_examples.png', dpi=300)\n",
        "    plt.close()\n",
        "    print(\"Combined visualization saved to 'patch_attack_examples.png'\")"
      ],
      "metadata": {
        "id": "G_BWE_hTlLwb"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset for the attack\n",
        "attack_dataset = FGSMDataset(\n",
        "    root_dir=dataset_path,\n",
        "    folder_to_label=folder_to_label,\n",
        "    transform=plain_transforms\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFXHhxmdlPHA",
        "outputId": "2d3b9320-7d36-4438-84bb-de8ce684b8f2"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 500 images from 100 class folders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use batch size of 1 for attack implementation\n",
        "attack_loader = DataLoader(attack_dataset, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "TIPe3IPHlRz7"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set attack parameters\n",
        "patch_size = 32    # Size of the square patch\n",
        "epsilon = 0.5      # Maximum perturbation (increased significantly)\n",
        "alpha = 0.1        # Step size\n",
        "num_iter = 100     # More iterations for better convergence\n"
      ],
      "metadata": {
        "id": "w9DAQQKQlUZB"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize counters for statistics\n",
        "total_images = 0\n",
        "successful_attacks = 0\n",
        "correct_original = 0\n",
        "correct_adversarial_top1 = 0\n",
        "correct_adversarial_top5 = 0\n",
        "max_patch_l_inf = 0\n",
        "avg_patch_l_inf = 0"
      ],
      "metadata": {
        "id": "Zjl75xjflXGc"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store visualization examples\n",
        "examples_to_visualize = []"
      ],
      "metadata": {
        "id": "TsY3k3AulabP"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Run Patch Attack and Save Results\n",
        "\n",
        "This loop runs optimized_patch_attack(...) for each image and saves the results and metrics."
      ],
      "metadata": {
        "id": "TcHfW-qghcwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform optimized patch attack on each image\n",
        "print(\"Starting optimized patch attacks...\")\n",
        "for i, (image, label, folder, img_name) in enumerate(attack_loader):\n",
        "    # Send data to device\n",
        "    image, label = image.to(device), label.to(device)\n",
        "\n",
        "    # Get original prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        _, original_pred = torch.max(output, 1)\n",
        "\n",
        "    original_correct = (original_pred == label).item()\n",
        "    if original_correct:\n",
        "        correct_original += 1\n",
        "\n",
        "    # Perform optimized patch attack\n",
        "    perturbed_image, patch_mask = optimized_patch_attack(\n",
        "        model=model,\n",
        "        image=image,\n",
        "        label=label,\n",
        "        patch_size=patch_size,\n",
        "        epsilon=epsilon,\n",
        "        alpha=alpha,\n",
        "        num_iter=num_iter\n",
        "    )\n",
        "\n",
        "    # Calculate L∞ distance in the patch region\n",
        "    l_inf = patch_l_inf_distance(image, perturbed_image, patch_mask)\n",
        "    max_patch_l_inf = max(max_patch_l_inf, l_inf)\n",
        "    avg_patch_l_inf += l_inf\n",
        "\n",
        "    # Forward pass with perturbed image\n",
        "    with torch.no_grad():\n",
        "        output_adv = model(perturbed_image)\n",
        "\n",
        "    # Get adversarial prediction\n",
        "    _, adv_pred = torch.max(output_adv, 1)\n",
        "\n",
        "    # Check if attack was successful in changing prediction\n",
        "    attack_success = (original_pred != adv_pred).item()\n",
        "    if attack_success:\n",
        "        successful_attacks += 1\n",
        "\n",
        "    # Check if adversarial example is still classified correctly (top-1)\n",
        "    adversarial_correct_top1 = (adv_pred == label).item()\n",
        "    if adversarial_correct_top1:\n",
        "        correct_adversarial_top1 += 1\n",
        "\n",
        "    # Check top-5 accuracy\n",
        "    _, top5_indices = output_adv.topk(5, dim=1)\n",
        "    adversarial_correct_top5 = label.item() in top5_indices.squeeze().tolist()\n",
        "    if adversarial_correct_top5:\n",
        "        correct_adversarial_top5 += 1\n",
        "\n",
        "    # Save the adversarial image\n",
        "    save_path = os.path.join(SAVE_PATH, folder[0], img_name[0])\n",
        "    adv_img_to_save = perturbed_image.detach().cpu().squeeze(0)\n",
        "    save_normalized_image(adv_img_to_save, save_path, mean_norms, std_norms)\n",
        "\n",
        "    # Collect examples for visualization (original correct but adversarial wrong)\n",
        "    if original_correct and attack_success and len(examples_to_visualize) < 5:\n",
        "        examples_to_visualize.append({\n",
        "            'original': image.detach(),\n",
        "            'adversarial': perturbed_image.detach(),\n",
        "            'patch_mask': patch_mask.detach(),\n",
        "            'original_label': original_pred.item(),\n",
        "            'adversarial_label': adv_pred.item(),\n",
        "            'true_label': label.item(),\n",
        "            'index': len(examples_to_visualize) + 1\n",
        "        })\n",
        "\n",
        "    # Update counter\n",
        "    total_images += 1\n",
        "\n",
        "    # Print progress\n",
        "    if (i + 1) % 50 == 0:\n",
        "        print(f\"Processed {i + 1}/{len(attack_loader)} images\")\n",
        "        # Print intermediate results\n",
        "        if total_images > 0:\n",
        "            interim_attack_rate = 100 * successful_attacks / total_images\n",
        "            print(f\"Current attack success rate: {interim_attack_rate:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7PgrpmPlcsB",
        "outputId": "24823a55-190a-4f92-c17b-c0375d9ca55b"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting optimized patch attacks...\n",
            "Processed 50/500 images\n",
            "Current attack success rate: 90.00%\n",
            "Processed 100/500 images\n",
            "Current attack success rate: 92.00%\n",
            "Processed 150/500 images\n",
            "Current attack success rate: 94.00%\n",
            "Processed 200/500 images\n",
            "Current attack success rate: 93.50%\n",
            "Processed 250/500 images\n",
            "Current attack success rate: 93.60%\n",
            "Processed 300/500 images\n",
            "Current attack success rate: 93.67%\n",
            "Processed 350/500 images\n",
            "Current attack success rate: 93.71%\n",
            "Processed 400/500 images\n",
            "Current attack success rate: 93.75%\n",
            "Processed 450/500 images\n",
            "Current attack success rate: 93.56%\n",
            "Processed 500/500 images\n",
            "Current attack success rate: 93.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 14: Compute Patch Attack Metrics\n",
        "\n",
        "Final statistics include attack success rate, accuracy drop, and average patch distortion."
      ],
      "metadata": {
        "id": "zHk_2cT8mtOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate final statistics\n",
        "avg_patch_l_inf = avg_patch_l_inf / total_images\n",
        "attack_success_rate = 100 * successful_attacks / total_images\n",
        "original_acc_top1 = 100 * correct_original / total_images\n",
        "adversarial_acc_top1 = 100 * correct_adversarial_top1 / total_images\n",
        "adversarial_acc_top5 = 100 * correct_adversarial_top5 / total_images\n",
        "relative_drop = 100 * (original_acc_top1 - adversarial_acc_top1) / original_acc_top1\n",
        "\n",
        "# Print results\n",
        "print(f\"\\nOptimized Patch Attack Results (patch size = {patch_size}x{patch_size}, ε = {epsilon}):\")\n",
        "print(f\"Maximum L∞ distance in patch: {max_patch_l_inf:.6f}\")\n",
        "print(f\"Average L∞ distance in patch: {avg_patch_l_inf:.6f}\")\n",
        "print(f\"Attack success rate: {attack_success_rate:.2f}%\")\n",
        "print(\"\\nModel Accuracy:\")\n",
        "print(f\"Original Top-1: {original_acc_top1:.2f}%\")\n",
        "print(f\"Adversarial Top-1: {adversarial_acc_top1:.2f}%\")\n",
        "print(f\"Adversarial Top-5: {adversarial_acc_top5:.2f}%\")\n",
        "print(f\"Relative accuracy drop: {relative_drop:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAjzUPI4rLBf",
        "outputId": "54e5290d-8e48-43ac-8ab5-9032d7f25d5d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimized Patch Attack Results (patch size = 32x32, ε = 0.5):\n",
            "Maximum L∞ distance in patch: 0.500000\n",
            "Average L∞ distance in patch: 0.491455\n",
            "Attack success rate: 93.60%\n",
            "\n",
            "Model Accuracy:\n",
            "Original Top-1: 70.60%\n",
            "Adversarial Top-1: 5.60%\n",
            "Adversarial Top-5: 81.60%\n",
            "Relative accuracy drop: 92.07%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 15: Visualize and Save Examples\n",
        "\n",
        "Display side-by-side examples to visually confirm localized perturbation impact."
      ],
      "metadata": {
        "id": "zFwOPRR0mzPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize examples\n",
        "print(\"\\nVisualizing attack examples...\")\n",
        "for example in examples_to_visualize:\n",
        "    original_class = get_class_name(example['true_label'])\n",
        "    adversarial_class = get_class_name(example['adversarial_label'])\n",
        "\n",
        "    visualize_patch_attack(\n",
        "        example['original'],\n",
        "        example['adversarial'],\n",
        "        example['patch_mask'],\n",
        "        example['original_label'],\n",
        "        example['adversarial_label'],\n",
        "        original_class,\n",
        "        adversarial_class,\n",
        "        example['index']\n",
        "    )\n",
        "\n",
        "print(f\"Saved {len(examples_to_visualize)} visualization examples\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_sqm8koqtVl",
        "outputId": "b9281dcc-d877-4f6f-d1a6-5a64c65eb0d9"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing attack examples...\n",
            "Saved 5 visualization examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a combined visualization\n",
        "if examples_to_visualize:\n",
        "    create_combined_patch_visualization(examples_to_visualize)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zukojWurSAL",
        "outputId": "34239ce9-5b81-43c7-9af9-bce8590e0af7"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined visualization saved to 'patch_attack_examples.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the mapping and labels to the adversarial dataset folder\n",
        "with open(os.path.join(SAVE_PATH, 'imagenet_mapping.json'), 'w') as f:\n",
        "    json.dump(mapping_dict, f, indent=4)"
      ],
      "metadata": {
        "id": "WRb7saLSrlcr"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the patch attack results\n",
        "results = {\n",
        "    'patch_size': patch_size,\n",
        "    'epsilon': epsilon,\n",
        "    'alpha': alpha,\n",
        "    'num_iter': num_iter,\n",
        "    'max_patch_l_inf': float(max_patch_l_inf),\n",
        "    'avg_patch_l_inf': float(avg_patch_l_inf),\n",
        "    'attack_success_rate': float(attack_success_rate),\n",
        "    'original_acc_top1': float(original_acc_top1),\n",
        "    'adversarial_acc_top1': float(adversarial_acc_top1),\n",
        "    'adversarial_acc_top5': float(adversarial_acc_top5),\n",
        "    'relative_drop': float(relative_drop)\n",
        "}\n",
        "\n",
        "with open('patch_attack_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(\"Results saved to 'patch_attack_results.json'\")"
      ],
      "metadata": {
        "id": "Z3toZkgsroIQ",
        "outputId": "36f43a74-8a6e-424d-e272-3a112504fbe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to 'patch_attack_results.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5: Transferability Analysis of Adversarial Attacks\n",
        "\n",
        " Task Objective\n",
        "\n",
        "In this final task, we test how well the adversarial examples generated for ResNet-34 (FGSM, PGD, Patch) transfer to other model architectures. This helps understand whether adversarial vulnerabilities are model-specific or universal."
      ],
      "metadata": {
        "id": "vpwF3SDRnEU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import Required Libraries\n",
        "\n",
        "This cell imports essential libraries for data loading, visualization, model evaluation, and analysis."
      ],
      "metadata": {
        "id": "_42aEnXHnZHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "YRjBDi83rtgj"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 2: Define Dataset Paths\n",
        "\n",
        "We define the paths to the original and all three adversarial test sets."
      ],
      "metadata": {
        "id": "QCoIw47gnbwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to all datasets\n",
        "ORIGINAL_PATH = \"./TestDataSet\"\n",
        "FGSM_PATH = \"./AdversarialTestSet1\"\n",
        "PGD_PATH = \"./AdversarialTestSet2\"\n",
        "PATCH_PATH = \"./AdversarialTestSet3\""
      ],
      "metadata": {
        "id": "bGzH3HMXsSds"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Load Class Mapping\n",
        "\n",
        "We load the previously saved mapping between folder names and label indices to ensure consistency.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eyeYCCCHnemP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we have the mapping file\n",
        "try:\n",
        "    with open('imagenet_mapping.json', 'r') as f:\n",
        "        mapping_dict = json.load(f)\n",
        "    folder_to_label = mapping_dict['folder_to_label']\n",
        "except:\n",
        "    print(\"Warning: Could not load imagenet_mapping.json, results may be incorrect\")\n",
        "    folder_to_label = {}"
      ],
      "metadata": {
        "id": "RiWzgXmesVET"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 4: Define Image Transformations\n",
        "\n",
        "We reuse the same preprocessing transforms as used in earlier tasks to ensure consistency across evaluations."
      ],
      "metadata": {
        "id": "X-Bwe40jniFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform for preprocessing images\n",
        "mean_norms = np.array([0.485, 0.456, 0.406])\n",
        "std_norms = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "plain_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean_norms, std=std_norms)\n",
        "])\n"
      ],
      "metadata": {
        "id": "vxKiVgVKsbtV"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Define Dataset Loader Class\n",
        "\n",
        "This class loads images from the dataset folders and applies the required transforms."
      ],
      "metadata": {
        "id": "mhiMF6nMnlVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class for loading images\n",
        "class OrderedImageNetDataset(Dataset):\n",
        "    def __init__(self, root_dir, folder_to_label, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.folder_to_label = folder_to_label\n",
        "        self.transform = transform\n",
        "\n",
        "        # Collect all image paths and their labels\n",
        "        self.samples = []\n",
        "        for folder, label in folder_to_label.items():\n",
        "            folder_path = os.path.join(root_dir, folder)\n",
        "            if os.path.isdir(folder_path):\n",
        "                for img_name in os.listdir(folder_path):\n",
        "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        img_path = os.path.join(folder_path, img_name)\n",
        "                        self.samples.append((img_path, label))\n",
        "\n",
        "        print(f\"Loaded {len(self.samples)} images from {len(folder_to_label)} class folders\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "zykfaLgjseUO"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Load All Models for Transferability\n",
        "\n",
        "We use pretrained models from torchvision: ResNet34, DenseNet121, VGG16, MobileNetV2, and EfficientNet-B0."
      ],
      "metadata": {
        "id": "AlIP1I31ns9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct_top1 = 0\n",
        "    correct_top5 = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Top-1 accuracy\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct_top1 += (predicted == labels).sum().item()\n",
        "\n",
        "            # Top-5 accuracy\n",
        "            _, top5_indices = outputs.topk(5, dim=1)\n",
        "            for i in range(labels.size(0)):\n",
        "                if labels[i] in top5_indices[i]:\n",
        "                    correct_top5 += 1\n",
        "\n",
        "            total += labels.size(0)\n",
        "\n",
        "    top1_accuracy = 100 * correct_top1 / total\n",
        "    top5_accuracy = 100 * correct_top5 / total\n",
        "\n",
        "    return top1_accuracy, top5_accuracy"
      ],
      "metadata": {
        "id": "dVWlar8mshTI"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 7: Evaluate Model on a Dataset\n",
        "\n",
        "Computes Top-1 and Top-5 accuracy for a given model and dataset."
      ],
      "metadata": {
        "id": "akXGUXR9nwb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load different model architectures for testing transferability\n",
        "def load_models():\n",
        "    models = {}\n",
        "\n",
        "\n",
        "    # Different architectures to test transferability\n",
        "    models['DenseNet-121'] = torchvision.models.densenet121(weights='IMAGENET1K_V1')\n",
        "    models['VGG-16'] = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "    models['MobileNetV2'] = torchvision.models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
        "    models['EfficientNetB0'] = torchvision.models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
        "\n",
        "    # Set all models to evaluation mode\n",
        "    for model_name, model in models.items():\n",
        "        model.eval()\n",
        "\n",
        "    return models"
      ],
      "metadata": {
        "id": "AmRqpX3osj8Y"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10: Create Dataloaders for All Datasets\n",
        "\n",
        "This function prepares dataloaders for the original and adversarial datasets (FGSM, PGD, Patch) using consistent transforms and mappings.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TCGzy30Jn5gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders for all datasets\n",
        "def create_dataloaders():\n",
        "    dataloaders = {}\n",
        "\n",
        "    # Original dataset\n",
        "    if os.path.exists(ORIGINAL_PATH):\n",
        "        original_dataset = OrderedImageNetDataset(\n",
        "            root_dir=ORIGINAL_PATH,\n",
        "            folder_to_label=folder_to_label,\n",
        "            transform=plain_transforms\n",
        "        )\n",
        "        dataloaders['Original'] = DataLoader(original_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # FGSM adversarial dataset\n",
        "    if os.path.exists(FGSM_PATH):\n",
        "        fgsm_dataset = OrderedImageNetDataset(\n",
        "            root_dir=FGSM_PATH,\n",
        "            folder_to_label=folder_to_label,\n",
        "            transform=plain_transforms\n",
        "        )\n",
        "        dataloaders['FGSM'] = DataLoader(fgsm_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # PGD adversarial dataset\n",
        "    if os.path.exists(PGD_PATH):\n",
        "        pgd_dataset = OrderedImageNetDataset(\n",
        "            root_dir=PGD_PATH,\n",
        "            folder_to_label=folder_to_label,\n",
        "            transform=plain_transforms\n",
        "        )\n",
        "        dataloaders['PGD'] = DataLoader(pgd_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Patch adversarial dataset\n",
        "    if os.path.exists(PATCH_PATH):\n",
        "        patch_dataset = OrderedImageNetDataset(\n",
        "            root_dir=PATCH_PATH,\n",
        "            folder_to_label=folder_to_label,\n",
        "            transform=plain_transforms\n",
        "        )\n",
        "        dataloaders['Patch'] = DataLoader(patch_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    return dataloaders\n"
      ],
      "metadata": {
        "id": "RrWvY6eesnKZ"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 11: Run Evaluation Across All Models and Datasets\n",
        "\n",
        "This version of main() uses create_dataloaders() for clean separation of concerns."
      ],
      "metadata": {
        "id": "lCQb3diloAI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate all models on all datasets\n",
        "def evaluate_transferability():\n",
        "    # Load all models\n",
        "    print(\"Loading models...\")\n",
        "    models = load_models()\n",
        "\n",
        "    # Create dataloaders\n",
        "    print(\"Loading datasets...\")\n",
        "    dataloaders = create_dataloaders()\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Move models to device\n",
        "    for model_name, model in models.items():\n",
        "        models[model_name] = model.to(device)\n",
        "\n",
        "    # Results will be stored in this dictionary\n",
        "    results = {\n",
        "        'Model': [],\n",
        "        'Dataset': [],\n",
        "        'Top-1 Accuracy': [],\n",
        "        'Top-5 Accuracy': []\n",
        "    }\n",
        "\n",
        "    # Evaluate each model on each dataset\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\nEvaluating {model_name}...\")\n",
        "\n",
        "        for dataset_name, dataloader in dataloaders.items():\n",
        "            print(f\"  Testing on {dataset_name} dataset...\")\n",
        "            top1_acc, top5_acc = evaluate_model(model, dataloader, device)\n",
        "\n",
        "            # Store results\n",
        "            results['Model'].append(model_name)\n",
        "            results['Dataset'].append(dataset_name)\n",
        "            results['Top-1 Accuracy'].append(top1_acc)\n",
        "            results['Top-5 Accuracy'].append(top5_acc)\n",
        "\n",
        "            print(f\"    Top-1 Accuracy: {top1_acc:.2f}%\")\n",
        "            print(f\"    Top-5 Accuracy: {top5_acc:.2f}%\")\n",
        "\n",
        "    # Convert results to DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "qvTgqdSXsqA8"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 12: Analyze Transferability Results\n",
        "\n",
        "This function:\n",
        "\n",
        "Computes relative accuracy drops from clean data to adversarial inputs,\n",
        "\n",
        "Summarizes attack effectiveness per model and per method,\n",
        "\n",
        "Displays a formatted table of accuracy drop (%) for each model across FGSM, PGD, and Patch attacks."
      ],
      "metadata": {
        "id": "sAhBEuWvoK6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_transferability(results_df):\n",
        "    print(\"\\n--- Transferability Analysis ---\")\n",
        "\n",
        "    # Remove ResNet-34 from the analysis\n",
        "    transferability_df = results_df[results_df['Model'] != 'ResNet-34'].copy()\n",
        "\n",
        "    # Add a column for accuracy drop\n",
        "    transferability_df['Accuracy Drop'] = 0.0\n",
        "\n",
        "    # For each model and attack, calculate the drop from original accuracy\n",
        "    for model in transferability_df['Model'].unique():\n",
        "        # Get original accuracy for this model\n",
        "        original_acc = transferability_df[(transferability_df['Model'] == model) &\n",
        "                                         (transferability_df['Dataset'] == 'Original')]['Top-1 Accuracy'].values[0]\n",
        "\n",
        "        # Calculate drop for each attack\n",
        "        for attack in ['FGSM', 'PGD', 'Patch']:\n",
        "            attack_acc = transferability_df[(transferability_df['Model'] == model) &\n",
        "                                           (transferability_df['Dataset'] == attack)]['Top-1 Accuracy'].values[0]\n",
        "\n",
        "            # Calculate absolute and relative drop\n",
        "            absolute_drop = original_acc - attack_acc\n",
        "            relative_drop = (absolute_drop / original_acc) * 100\n",
        "\n",
        "            # Store in dataframe\n",
        "            idx = (transferability_df['Model'] == model) & (transferability_df['Dataset'] == attack)\n",
        "            transferability_df.loc[idx, 'Accuracy Drop'] = relative_drop\n",
        "\n",
        "    # Display results by attack type\n",
        "    print(\"\\nAccuracy Drop by Attack Type (higher = more effective transfer):\")\n",
        "    for attack in ['FGSM', 'PGD', 'Patch']:\n",
        "        print(f\"\\n{attack} Attack Transferability:\")\n",
        "\n",
        "        attack_data = transferability_df[transferability_df['Dataset'] == attack]\n",
        "\n",
        "        for model in attack_data['Model'].unique():\n",
        "            drop = attack_data[attack_data['Model'] == model]['Accuracy Drop'].values[0]\n",
        "            attack_success_rate = 100 - attack_data[attack_data['Model'] == model]['Top-1 Accuracy'].values[0]\n",
        "\n",
        "            print(f\"  {model}: {drop:.2f}% accuracy drop ({attack_success_rate:.2f}% attack success rate)\")\n",
        "\n",
        "    # Display results by model\n",
        "    print(\"\\nModel Vulnerability to Transferred Attacks:\")\n",
        "    for model in transferability_df['Model'].unique():\n",
        "        model_data = transferability_df[(transferability_df['Model'] == model) &\n",
        "                                       (transferability_df['Dataset'] != 'Original')]\n",
        "\n",
        "        avg_drop = model_data['Accuracy Drop'].mean()\n",
        "        print(f\"  {model}: {avg_drop:.2f}% average accuracy drop\")\n",
        "\n",
        "    # Calculate average transferability for each attack\n",
        "    print(\"\\nAverage Transferability Across All Models:\")\n",
        "    for attack in ['FGSM', 'PGD', 'Patch']:\n",
        "        attack_data = transferability_df[transferability_df['Dataset'] == attack]\n",
        "        avg_drop = attack_data['Accuracy Drop'].mean()\n",
        "\n",
        "        print(f\"  {attack}: {avg_drop:.2f}% average accuracy drop\")\n",
        "\n",
        "    # Create a summary table\n",
        "    print(\"\\nSummary Table (Accuracy Drop %):\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Model':<15} {'FGSM':<10} {'PGD':<10} {'Patch':<10} {'Average':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for model in sorted(transferability_df['Model'].unique()):\n",
        "        fgsm_drop = transferability_df[(transferability_df['Model'] == model) &\n",
        "                                      (transferability_df['Dataset'] == 'FGSM')]['Accuracy Drop'].values[0]\n",
        "        pgd_drop = transferability_df[(transferability_df['Model'] == model) &\n",
        "                                     (transferability_df['Dataset'] == 'PGD')]['Accuracy Drop'].values[0]\n",
        "        patch_drop = transferability_df[(transferability_df['Model'] == model) &\n",
        "                                       (transferability_df['Dataset'] == 'Patch')]['Accuracy Drop'].values[0]\n",
        "        avg = (fgsm_drop + pgd_drop + patch_drop) / 3\n",
        "\n",
        "        print(f\"{model:<15} {fgsm_drop:<10.2f} {pgd_drop:<10.2f} {patch_drop:<10.2f} {avg:<10.2f}\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Calculate averages for each attack\n",
        "    fgsm_avg = transferability_df[transferability_df['Dataset'] == 'FGSM']['Accuracy Drop'].mean()\n",
        "    pgd_avg = transferability_df[transferability_df['Dataset'] == 'PGD']['Accuracy Drop'].mean()\n",
        "    patch_avg = transferability_df[transferability_df['Dataset'] == 'Patch']['Accuracy Drop'].mean()\n",
        "    total_avg = (fgsm_avg + pgd_avg + patch_avg) / 3\n",
        "\n",
        "    print(f\"{'Average':<15} {fgsm_avg:<10.2f} {pgd_avg:<10.2f} {patch_avg:<10.2f} {total_avg:<10.2f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    return transferability_df"
      ],
      "metadata": {
        "id": "3bJxXVhrvHQQ"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 13: Visualize Transferability Results\n",
        "\n",
        "This function generates three clean visualizations:\n",
        "\n",
        "Heatmaps of Top-1 and Top-5 accuracy\n",
        "\n",
        "Bar plots comparing accuracy across models and datasets\n",
        "\n",
        "Line plot of relative accuracy drop (effectiveness of each attack)"
      ],
      "metadata": {
        "id": "c3Jz6WtgoU0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(results_df):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "\n",
        "    # 1. Top-1 and Top-5 Accuracy heatmaps\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    top1_pivot = results_df.pivot_table(\n",
        "        values='Top-1 Accuracy',\n",
        "        index='Model',\n",
        "        columns='Dataset'\n",
        "    )\n",
        "    sns.heatmap(top1_pivot, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar_kws={'label': 'Accuracy (%)'})\n",
        "    plt.title('Top-1 Accuracy Across Models and Datasets')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    top5_pivot = results_df.pivot_table(\n",
        "        values='Top-5 Accuracy',\n",
        "        index='Model',\n",
        "        columns='Dataset'\n",
        "    )\n",
        "    sns.heatmap(top5_pivot, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar_kws={'label': 'Accuracy (%)'})\n",
        "    plt.title('Top-5 Accuracy Across Models and Datasets')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('transferability_heatmap.png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Bar plots\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    models = results_df['Model'].unique()\n",
        "    bar_width = 0.2\n",
        "    index = np.arange(len(models))\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    for i, dataset in enumerate(['Original', 'FGSM', 'PGD', 'Patch']):\n",
        "        dataset_results = results_df[results_df['Dataset'] == dataset]\n",
        "        dataset_results = dataset_results.set_index('Model').reindex(models)\n",
        "        plt.bar(index + i*bar_width, dataset_results['Top-1 Accuracy'], bar_width, label=dataset)\n",
        "\n",
        "    plt.xlabel('Model Architecture')\n",
        "    plt.ylabel('Top-1 Accuracy (%)')\n",
        "    plt.title('Top-1 Accuracy Comparison Across Models and Attacks')\n",
        "    plt.xticks(index + bar_width * 1.5, models, rotation=45, ha='right')\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    for i, dataset in enumerate(['Original', 'FGSM', 'PGD', 'Patch']):\n",
        "        dataset_results = results_df[results_df['Dataset'] == dataset]\n",
        "        dataset_results = dataset_results.set_index('Model').reindex(models)\n",
        "        plt.bar(index + i*bar_width, dataset_results['Top-5 Accuracy'], bar_width, label=dataset)\n",
        "\n",
        "    plt.xlabel('Model Architecture')\n",
        "    plt.ylabel('Top-5 Accuracy (%)')\n",
        "    plt.title('Top-5 Accuracy Comparison Across Models and Attacks')\n",
        "    plt.xticks(index + bar_width * 1.5, models, rotation=45, ha='right')\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('transferability_barplot.png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Line plot: Attack Effectiveness (Accuracy Drop)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    drops = []\n",
        "    models_list = []\n",
        "    attacks_list = []\n",
        "\n",
        "    for model in results_df['Model'].unique():\n",
        "        original_acc = results_df[(results_df['Model'] == model) &\n",
        "                                  (results_df['Dataset'] == 'Original')]['Top-1 Accuracy'].values[0]\n",
        "\n",
        "        for attack in ['FGSM', 'PGD', 'Patch']:\n",
        "            attack_acc = results_df[(results_df['Model'] == model) &\n",
        "                                    (results_df['Dataset'] == attack)]['Top-1 Accuracy'].values[0]\n",
        "            drop = (original_acc - attack_acc) / original_acc * 100\n",
        "\n",
        "            drops.append(drop)\n",
        "            models_list.append(model)\n",
        "            attacks_list.append(attack)\n",
        "\n",
        "    drops_df = pd.DataFrame({\n",
        "        'Model': models_list,\n",
        "        'Attack': attacks_list,\n",
        "        'Accuracy Drop (%)': drops\n",
        "    })\n",
        "\n",
        "    for attack in ['FGSM', 'PGD', 'Patch']:\n",
        "        attack_data = drops_df[drops_df['Attack'] == attack]\n",
        "        plt.plot(attack_data['Model'], attack_data['Accuracy Drop (%)'],\n",
        "                 marker='o', label=attack)\n",
        "\n",
        "    plt.grid(linestyle='--', alpha=0.7)\n",
        "    plt.xlabel('Model Architecture')\n",
        "    plt.ylabel('Accuracy Drop (%)')\n",
        "    plt.title('Attack Effectiveness Across Model Architectures')\n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('attack_effectiveness.png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Visualizations saved to 'transferability_heatmap.png', 'transferability_barplot.png', and 'attack_effectiveness.png'\")\n"
      ],
      "metadata": {
        "id": "vLbWFYQWvQS9"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 14: Main Function – Run the Entire Transferability Pipeline\n",
        "\n",
        "This function orchestrates:\n",
        "\n",
        "Accuracy evaluation across all models & datasets\n",
        "\n",
        "CSV saving of raw results\n",
        "\n",
        "Analysis of cross-model attack transferability\n",
        "\n",
        "Generation of plots and insights for reporting"
      ],
      "metadata": {
        "id": "9NAeyUL8oa7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to run the entire analysis\n",
        "def main():\n",
        "    print(\"Starting transferability analysis (Task 5)...\")\n",
        "\n",
        "    # Evaluate transferability\n",
        "    results_df = evaluate_transferability()\n",
        "\n",
        "    # Save results to CSV\n",
        "    results_df.to_csv('transferability_results.csv', index=False)\n",
        "    print(\"Results saved to 'transferability_results.csv'\")\n",
        "\n",
        "    # Analyze transferability\n",
        "    analyze_transferability(results_df)\n",
        "\n",
        "    # Visualize results\n",
        "    visualize_results(results_df)\n",
        "\n",
        "    print(\"\\nTransferability analysis complete!\")\n",
        "\n",
        "    # Print summary and insights\n",
        "    print(\"\\n--- Summary and Insights ---\")\n",
        "    print(\"1. Transferability between architectures: Different model architectures exhibit varying levels of robustness to adversarial attacks generated for ResNet-34.\")\n",
        "    print(\"2. Attack effectiveness comparison: PGD attacks generally transfer better than FGSM attacks, while patch attacks may show unique transferability patterns.\")\n",
        "    print(\"3. Model robustness: Some architectures appear more naturally robust to transferred attacks than others.\")\n",
        "    print(\"4. Mitigation strategies: These results suggest potential approaches for improving model robustness, such as ensemble methods and adversarial training.\")\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "ib5XToB-veJG"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the analysis if this script is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    results_df = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FywQRnrgvg1x",
        "outputId": "2933ca4a-ab7f-4b89-a3fc-091a2c22467d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting transferability analysis (Task 5)...\n",
            "Loading models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 204MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:02<00:00, 214MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 164MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 192MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "Loaded 500 images from 100 class folders\n",
            "Loaded 500 images from 100 class folders\n",
            "Loaded 500 images from 100 class folders\n",
            "Loaded 500 images from 100 class folders\n",
            "Using device: cuda\n",
            "\n",
            "Evaluating DenseNet-121...\n",
            "  Testing on Original dataset...\n",
            "    Top-1 Accuracy: 70.80%\n",
            "    Top-5 Accuracy: 91.20%\n",
            "  Testing on FGSM dataset...\n",
            "    Top-1 Accuracy: 61.60%\n",
            "    Top-5 Accuracy: 85.20%\n",
            "  Testing on PGD dataset...\n",
            "    Top-1 Accuracy: 63.80%\n",
            "    Top-5 Accuracy: 86.60%\n",
            "  Testing on Patch dataset...\n",
            "    Top-1 Accuracy: 64.00%\n",
            "    Top-5 Accuracy: 85.60%\n",
            "\n",
            "Evaluating VGG-16...\n",
            "  Testing on Original dataset...\n",
            "    Top-1 Accuracy: 66.80%\n",
            "    Top-5 Accuracy: 91.20%\n",
            "  Testing on FGSM dataset...\n",
            "    Top-1 Accuracy: 55.20%\n",
            "    Top-5 Accuracy: 81.60%\n",
            "  Testing on PGD dataset...\n",
            "    Top-1 Accuracy: 56.60%\n",
            "    Top-5 Accuracy: 81.40%\n",
            "  Testing on Patch dataset...\n",
            "    Top-1 Accuracy: 57.40%\n",
            "    Top-5 Accuracy: 81.20%\n",
            "\n",
            "Evaluating MobileNetV2...\n",
            "  Testing on Original dataset...\n",
            "    Top-1 Accuracy: 68.20%\n",
            "    Top-5 Accuracy: 89.20%\n",
            "  Testing on FGSM dataset...\n",
            "    Top-1 Accuracy: 58.80%\n",
            "    Top-5 Accuracy: 82.60%\n",
            "  Testing on PGD dataset...\n",
            "    Top-1 Accuracy: 59.80%\n",
            "    Top-5 Accuracy: 84.80%\n",
            "  Testing on Patch dataset...\n",
            "    Top-1 Accuracy: 60.00%\n",
            "    Top-5 Accuracy: 82.80%\n",
            "\n",
            "Evaluating EfficientNetB0...\n",
            "  Testing on Original dataset...\n",
            "    Top-1 Accuracy: 79.80%\n",
            "    Top-5 Accuracy: 97.00%\n",
            "  Testing on FGSM dataset...\n",
            "    Top-1 Accuracy: 70.20%\n",
            "    Top-5 Accuracy: 92.80%\n",
            "  Testing on PGD dataset...\n",
            "    Top-1 Accuracy: 72.60%\n",
            "    Top-5 Accuracy: 93.80%\n",
            "  Testing on Patch dataset...\n",
            "    Top-1 Accuracy: 71.40%\n",
            "    Top-5 Accuracy: 93.60%\n",
            "Results saved to 'transferability_results.csv'\n",
            "\n",
            "--- Transferability Analysis ---\n",
            "\n",
            "Accuracy Drop by Attack Type (higher = more effective transfer):\n",
            "\n",
            "FGSM Attack Transferability:\n",
            "  DenseNet-121: 12.99% accuracy drop (38.40% attack success rate)\n",
            "  VGG-16: 17.37% accuracy drop (44.80% attack success rate)\n",
            "  MobileNetV2: 13.78% accuracy drop (41.20% attack success rate)\n",
            "  EfficientNetB0: 12.03% accuracy drop (29.80% attack success rate)\n",
            "\n",
            "PGD Attack Transferability:\n",
            "  DenseNet-121: 9.89% accuracy drop (36.20% attack success rate)\n",
            "  VGG-16: 15.27% accuracy drop (43.40% attack success rate)\n",
            "  MobileNetV2: 12.32% accuracy drop (40.20% attack success rate)\n",
            "  EfficientNetB0: 9.02% accuracy drop (27.40% attack success rate)\n",
            "\n",
            "Patch Attack Transferability:\n",
            "  DenseNet-121: 9.60% accuracy drop (36.00% attack success rate)\n",
            "  VGG-16: 14.07% accuracy drop (42.60% attack success rate)\n",
            "  MobileNetV2: 12.02% accuracy drop (40.00% attack success rate)\n",
            "  EfficientNetB0: 10.53% accuracy drop (28.60% attack success rate)\n",
            "\n",
            "Model Vulnerability to Transferred Attacks:\n",
            "  DenseNet-121: 10.83% average accuracy drop\n",
            "  VGG-16: 15.57% average accuracy drop\n",
            "  MobileNetV2: 12.71% average accuracy drop\n",
            "  EfficientNetB0: 10.53% average accuracy drop\n",
            "\n",
            "Average Transferability Across All Models:\n",
            "  FGSM: 14.04% average accuracy drop\n",
            "  PGD: 11.62% average accuracy drop\n",
            "  Patch: 11.56% average accuracy drop\n",
            "\n",
            "Summary Table (Accuracy Drop %):\n",
            "------------------------------------------------------------\n",
            "Model           FGSM       PGD        Patch      Average   \n",
            "------------------------------------------------------------\n",
            "DenseNet-121    12.99      9.89       9.60       10.83     \n",
            "EfficientNetB0  12.03      9.02       10.53      10.53     \n",
            "MobileNetV2     13.78      12.32      12.02      12.71     \n",
            "VGG-16          17.37      15.27      14.07      15.57     \n",
            "------------------------------------------------------------\n",
            "Average         14.04      11.62      11.56      12.41     \n",
            "------------------------------------------------------------\n",
            "Visualizations saved to 'transferability_heatmap.png', 'transferability_barplot.png', and 'attack_effectiveness.png'\n",
            "\n",
            "Transferability analysis complete!\n",
            "\n",
            "--- Summary and Insights ---\n",
            "1. Transferability between architectures: Different model architectures exhibit varying levels of robustness to adversarial attacks generated for ResNet-34.\n",
            "2. Attack effectiveness comparison: PGD attacks generally transfer better than FGSM attacks, while patch attacks may show unique transferability patterns.\n",
            "3. Model robustness: Some architectures appear more naturally robust to transferred attacks than others.\n",
            "4. Mitigation strategies: These results suggest potential approaches for improving model robustness, such as ensemble methods and adversarial training.\n"
          ]
        }
      ]
    }
  ]
}